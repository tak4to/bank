# 特徴量選択の手法と根拠

## 目的

機械学習モデルの性能を維持しながら、不要な特徴量を削減することで：
- 過学習を防止
- 学習時間の短縮
- モデルの解釈性向上
- 汎化性能の向上

## 特徴量選択の基準

### 1. ターゲット変数との相関分析

**手法:**
- Pearson相関係数を計算
- 相関の絶対値が高い特徴量を優先

**根拠:**
- ターゲット変数と相関が高い特徴量は予測に有効
- 相関が低い（|r| < 0.01程度）特徴量は削除候補

**選択基準:** 相関係数の絶対値 Top 30

### 2. 特徴量重要度分析（LightGBM）

**手法:**
- LightGBMの`feature_importances_`を使用
- Gain（分岐による損失関数の改善量）ベースの重要度

**根拠:**
- 実際のモデルで使用される特徴量を定量的に評価
- ツリーベースモデルでの実効性を直接測定

**選択基準:** 重要度 Top 40

### 3. 多重共線性チェック（VIF）

**手法:**
- VIF (Variance Inflation Factor) を計算
- VIF = 1 / (1 - R²)

**根拠:**
- 特徴量間の相関が高いと（多重共線性）：
  - モデルが不安定になる
  - 係数の解釈が困難
  - 予測性能が低下する可能性

**判定基準:**
- VIF < 5: 問題なし
- 5 ≤ VIF < 10: 注意が必要
- VIF ≥ 10: 多重共線性の問題あり（削除候補）

**選択基準:** VIF < 10

## 総合的な選択ロジック

```
選択される特徴量 = (重要度 Top 40 OR 相関 Top 30) AND VIF < 10
```

**理由:**
1. **重要度または相関が高い**: 予測に貢献する可能性が高い
2. **VIFが低い**: 他の特徴量と独立した情報を持つ

## 特徴量カテゴリ

### 数値特徴量（選択対象）
- 元の数値特徴量: `age`, `balance`, `duration`, `campaign`, `pdays`, `previous`
- エンジニアリングした特徴量: `balance_log`, `duration_log`, `duration_per_day`, etc.

### カテゴリカル特徴量（すべて保持）
- ワンホットエンコーディング後の特徴量
- 理由: カテゴリ情報は本質的に独立しており、ドメイン知識として重要

含まれるカテゴリ:
- `job_*`: 職業
- `marital_*`: 婚姻状況
- `education_*`: 教育レベル
- `contact_*`: 連絡方法
- `poutcome_*`: 前回のキャンペーン結果
- `age_group_*`: 年齢グループ（16分割）
- `job_education_*`: 職業×教育の相互作用
- `contact_month_*`: 連絡方法×月の相互作用

## 実行方法

```bash
# 特徴量分析を実行
python src/feature_analysis.py
```

出力:
1. コンソールに分析結果を表示
2. `data/selected_features.txt` に選択された特徴量リストを保存

## 期待される効果

- **特徴量数の削減**: 約30-50%削減
- **学習時間の短縮**: 約20-40%短縮
- **性能への影響**: AUCスコアは±0.5%以内を維持
- **過学習の抑制**: より汎化性能の高いモデル

## 注意点

### カテゴリカル特徴量を保持する理由

1. **ドメイン知識**: 職業、教育、月などは銀行マーケティングにおいて重要な要素
2. **非線形性**: カテゴリ間の関係は非線形であり、数値化では失われる情報がある
3. **ワンホット化の特性**: 各カテゴリが独立したバイナリ変数となり、VIFは適用不可

### 相互作用特徴量

- `job_education`: 職業と教育レベルの組み合わせは顧客セグメントを表す
- `contact_month`: 連絡方法と月の組み合わせは季節性を捉える

これらは高次の情報を持つため、重要度が高い場合が多い
