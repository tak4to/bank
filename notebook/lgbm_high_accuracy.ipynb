{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高精度LightGBMモデル（目標: 99%精度）\n",
    "\n",
    "## データ分析から得られた重要な知見:\n",
    "- **duration（通話時間）が最重要特徴量**: y=1の平均547秒 vs y=0の平均223秒（2.5倍の差）\n",
    "- **不均衡データ**: y=1が11.7%のみ\n",
    "- **前回コンタクトの影響**: pdays!=-1の場合、成約率が2倍以上\n",
    "\n",
    "## 改善戦略:\n",
    "1. duration関連の特徴量を強化\n",
    "2. 不均衡データへの対応\n",
    "3. より効果的な特徴量エンジニアリング\n",
    "4. ハイパーパラメータの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['y'].value_counts())\n",
    "print(f\"Positive rate: {train_df['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_train=True, encoders=None):\n",
    "    \"\"\"\n",
    "    改良版特徴量エンジニアリング\n",
    "    \n",
    "    主な改善点:\n",
    "    - durationベースの特徴量を強化\n",
    "    - バグ修正（month列の扱い）\n",
    "    - より効果的なエンコーディング\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== 1. 数値特徴量の変換 =====\n",
    "    \n",
    "    # age関連\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                              bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                              labels=['0-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "    df['age_squared'] = df['age'] ** 2\n",
    "    \n",
    "    # balance関連\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\n",
    "    \n",
    "    # ===== 2. DURATION関連特徴量（最重要） =====\n",
    "    \n",
    "    # 通話時間の対数変換\n",
    "    df['duration_log'] = np.log1p(df['duration'])\n",
    "    \n",
    "    # 通話時間の2乗（非線形性を捉える）\n",
    "    df['duration_squared'] = df['duration'] ** 2\n",
    "    \n",
    "    # 通話時間の平方根\n",
    "    df['duration_sqrt'] = np.sqrt(df['duration'])\n",
    "    \n",
    "    # 通話時間のビン分割（カテゴリ化）\n",
    "    df['duration_bin'] = pd.cut(df['duration'], \n",
    "                                 bins=[-1, 100, 200, 300, 500, 1000, 10000],\n",
    "                                 labels=['very_short', 'short', 'medium', 'long', 'very_long', 'extremely_long'])\n",
    "    \n",
    "    # 1日あたりの通話時間\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n",
    "    \n",
    "    # キャンペーン効率（通話時間/キャンペーン回数）\n",
    "    df['duration_per_campaign'] = df['duration'] / (df['campaign'] + 1)\n",
    "    \n",
    "    # ===== 3. 月の特徴量 =====\n",
    "    \n",
    "    month_mapping = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }\n",
    "    df['month_numeric'] = df['month'].map(month_mapping)\n",
    "    \n",
    "    # 周期性エンコーディング\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\n",
    "    \n",
    "    # 四半期\n",
    "    df['quarter'] = ((df['month_numeric'] - 1) // 3) + 1\n",
    "    \n",
    "    # ===== 4. pdays/previous関連特徴量 =====\n",
    "    \n",
    "    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\n",
    "    df['pdays_log'] = np.log1p(df['pdays'].replace(-1, 0))\n",
    "    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\n",
    "    \n",
    "    # ===== 5. ローン関連特徴量 =====\n",
    "    \n",
    "    # yes/noを0/1に変換\n",
    "    for col in ['default', 'housing', 'loan']:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    df['total_loans'] = df['housing'] + df['loan']\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n",
    "    df['has_default'] = df['default']\n",
    "    \n",
    "    # ===== 6. 複合特徴量 =====\n",
    "    \n",
    "    # campaignとdurationの関係\n",
    "    df['campaign_intensity'] = df['campaign'] * df['duration']\n",
    "    \n",
    "    # dayとdurationの関係\n",
    "    df['day_duration_interaction'] = df['day'] * df['duration']\n",
    "    \n",
    "    # ===== 7. カテゴリカル変数のエンコーディング =====\n",
    "    \n",
    "    categorical_feats = ['job', 'marital', 'education', 'contact', 'poutcome', \n",
    "                         'age_group', 'duration_bin']\n",
    "    \n",
    "    if is_train:\n",
    "        target_encoders = {}\n",
    "        label_encoders = {}\n",
    "        freq_encoders = {}\n",
    "        \n",
    "        # Target Encoding\n",
    "        for col in categorical_feats:\n",
    "            if 'y' in df.columns:\n",
    "                target_mean = df.groupby(col)['y'].mean()\n",
    "                global_mean = df['y'].mean()\n",
    "                counts = df.groupby(col).size()\n",
    "                smoothing = 10\n",
    "                smooth_target = (target_mean * counts + global_mean * smoothing) / (counts + smoothing)\n",
    "                target_encoders[col] = smooth_target\n",
    "                df[f'{col}_target_enc'] = df[col].map(smooth_target)\n",
    "        \n",
    "        # Frequency Encoding\n",
    "        for col in categorical_feats:\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            freq_encoders[col] = freq\n",
    "            df[f'{col}_freq'] = df[col].map(freq)\n",
    "        \n",
    "        # Label Encoding\n",
    "        for col in categorical_feats:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # monthとmonth_numericを削除（既に特徴量化済み）\n",
    "        df = df.drop(columns=['month', 'month_numeric'])\n",
    "        \n",
    "        encoders = {\n",
    "            'target_encoders': target_encoders,\n",
    "            'label_encoders': label_encoders,\n",
    "            'freq_encoders': freq_encoders\n",
    "        }\n",
    "        return df, encoders\n",
    "    \n",
    "    else:\n",
    "        if encoders is None:\n",
    "            raise ValueError(\"テストデータ処理時にはencodersを渡す必要があります\")\n",
    "        \n",
    "        target_encoders = encoders['target_encoders']\n",
    "        label_encoders = encoders['label_encoders']\n",
    "        freq_encoders = encoders['freq_encoders']\n",
    "        \n",
    "        # Target Encoding\n",
    "        for col in categorical_feats:\n",
    "            if col in target_encoders:\n",
    "                df[f'{col}_target_enc'] = df[col].map(target_encoders[col])\n",
    "                df[f'{col}_target_enc'] = df[f'{col}_target_enc'].astype('float64')\n",
    "                df[f'{col}_target_enc'].fillna(target_encoders[col].mean(), inplace=True)\n",
    "        \n",
    "        # Frequency Encoding\n",
    "        for col in categorical_feats:\n",
    "            if col in freq_encoders:\n",
    "                df[f'{col}_freq'] = df[col].map(freq_encoders[col])\n",
    "                df[f'{col}_freq'].fillna(freq_encoders[col].min(), inplace=True)\n",
    "        \n",
    "        # Label Encoding\n",
    "        for col in categorical_feats:\n",
    "            if col in label_encoders:\n",
    "                le = label_encoders[col]\n",
    "                df[col] = df[col].astype(str).apply(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "        \n",
    "        # monthとmonth_numericを削除\n",
    "        df = df.drop(columns=['month', 'month_numeric'])\n",
    "        \n",
    "        return df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量エンジニアリングを適用\n",
    "train_processed, encoders = feature_engineering(train_df, is_train=True)\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"New train shape: {train_processed.shape}\")\n",
    "print(f\"\\nFeature columns: {list(train_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットと特徴量の分離\n",
    "y = train_processed['y']\n",
    "exclude_cols = ['id', 'y']\n",
    "X = train_processed.drop(columns=exclude_cols)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"\\nFirst few feature names: {list(X.columns[:20])}\")\n",
    "\n",
    "# Train/Valid分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Valid set: {X_valid.shape}\")\n",
    "print(f\"Train y=1 rate: {y_train.mean():.4f}\")\n",
    "print(f\"Valid y=1 rate: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化\n",
    "    accuracy（精度）を最大化する\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 3000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"class_weight\": \"balanced\",  # 不均衡データ対応\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.early_stopping(150, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # 予測\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 最適化実行\n",
    "print(\"ハイパーパラメータ最適化を開始...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest Accuracy: {study_lgb.best_value:.6f}\")\n",
    "print(f\"Best params: {study_lgb.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終モデルの学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適パラメータでモデル学習\n",
    "best_params_lgb = study_lgb.best_params\n",
    "best_params_lgb.update({\n",
    "    \"n_estimators\": 3000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"boosting_type\": \"gbdt\"\n",
    "})\n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(**best_params_lgb)\n",
    "model_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    callbacks=[lgb.early_stopping(150, verbose=True), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(\"\\nモデル学習完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データでの評価\n",
    "y_pred_proba = model_lgb.predict_proba(X_valid)[:, 1]\n",
    "y_pred = model_lgb.predict(X_valid)\n",
    "\n",
    "# 各種メトリクス\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred)\n",
    "recall = recall_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"検証データでの性能\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.6f} ({accuracy*100:.4f}%)\")\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall:    {recall:.6f}\")\n",
    "print(f\"F1 Score:  {f1:.6f}\")\n",
    "print(f\"AUC:       {auc:.6f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 混同行列\n",
    "print(\"\\n混同行列:\")\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# 詳細レポート\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の可視化\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model_lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 重要な特徴量:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(feature_importance.head(30)['feature'], feature_importance.head(30)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 30 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータへの予測と提出ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに特徴量エンジニアリングを適用\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False, encoders=encoders)\n",
    "\n",
    "# 特徴量の抽出\n",
    "X_test = test_processed.drop(columns=['id'])\n",
    "\n",
    "# 予測\n",
    "test_pred = model_lgb.predict(X_test)\n",
    "\n",
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/high_accuracy_submission.csv', index=False, header=False)\n",
    "print(\"提出ファイルを作成しました: ../data/high_accuracy_submission.csv\")\n",
    "print(f\"\\n予測されたy=1の割合: {test_pred.mean():.4f}\")\n",
    "print(f\"予測されたy=1の数: {test_pred.sum()}\")\n",
    "print(f\"予測されたy=0の数: {(1-test_pred).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# モデルとエンコーダーを保存\n",
    "with open('../data/lgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_lgb, f)\n",
    "\n",
    "with open('../data/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(\"モデルとエンコーダーを保存しました\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
