{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# バランスデータセット学習\n",
    "このノートブックでは、正例と負例が1:1になるようにデータセットをバランス調整して学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"/home/user/bank/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/user/bank/data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution (元データ):\")\n",
    "print(train_df['y'].value_counts())\n",
    "print(f\"\\nPositive rate: {train_df['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バランスデータセットの作成\n",
    "正例と負例が1:1になるように、アンダーサンプリングを実施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df, target_col='y', random_state=42):\n",
    "    \"\"\"\n",
    "    正例と負例が1:1になるようにバランス調整したデータセットを作成\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        元のデータフレーム\n",
    "    target_col : str\n",
    "        ターゲット列の名前\n",
    "    random_state : int\n",
    "        ランダムシード\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    balanced_df : DataFrame\n",
    "        バランス調整済みのデータフレーム\n",
    "    \"\"\"\n",
    "    # 正例と負例に分離\n",
    "    positive_samples = df[df[target_col] == 1]\n",
    "    negative_samples = df[df[target_col] == 0]\n",
    "    \n",
    "    print(f\"正例数: {len(positive_samples)}\")\n",
    "    print(f\"負例数: {len(negative_samples)}\")\n",
    "    \n",
    "    # 少数クラスの数を基準にする\n",
    "    min_samples = min(len(positive_samples), len(negative_samples))\n",
    "    \n",
    "    # 各クラスから同じ数だけランダムサンプリング\n",
    "    positive_balanced = positive_samples.sample(n=min_samples, random_state=random_state)\n",
    "    negative_balanced = negative_samples.sample(n=min_samples, random_state=random_state)\n",
    "    \n",
    "    # 結合してシャッフル\n",
    "    balanced_df = pd.concat([positive_balanced, negative_balanced], axis=0)\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nバランス調整後のデータセットサイズ: {len(balanced_df)}\")\n",
    "    print(f\"正例数: {(balanced_df[target_col] == 1).sum()}\")\n",
    "    print(f\"負例数: {(balanced_df[target_col] == 0).sum()}\")\n",
    "    print(f\"正例率: {balanced_df[target_col].mean():.4f}\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# バランスデータセットを作成\n",
    "train_balanced = create_balanced_dataset(train_df, target_col='y', random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def feature_engineering(df, is_train=True):\n    \"\"\"\n    特徴量エンジニアリング関数（ワンホットエンコーディング版）\n    \"\"\"\n    df = df.copy()\n    \n    # ===== 1. 数値特徴量の変換 =====\n    df['age_group'] = pd.cut(df['age'], bins=16).astype(str)\n    # LightGBMはJSON特殊文字をサポートしないため、すべて置換\n    df['age_group'] = df['age_group'].str.replace(r'[\\(\\)\\[\\]\\{\\}\\:\\\"\\,\\.\\s]', '_', regex=True)\n    \n    # balance の対数変換\n    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n    df['balance_positive'] = (df['balance'] > 0).astype(int)\n    df['balance_negative'] = (df['balance'] < 0).astype(int)\n    \n    # ===== 2. 時系列特徴量 =====\n    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\n    df['duration_log'] = np.log1p(df['duration'])\n    \n    # previous関連\n    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\n    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\n    \n    # ===== 3. 月のマッピングと周期性エンコーディング =====\n    month_mapping = {\n        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n    }\n    df['month_numeric'] = df['month'].map(month_mapping)\n    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\n    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\n    \n    # ===== 4. ローン関連の特徴量 =====\n    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\n    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n    \n    # ===== 5. カテゴリカル特徴量の準備 =====\n    binary_cols = ['default', 'housing', 'loan']\n    for col in binary_cols:\n        df[col] = df[col].map({'yes': 1, 'no': 0})\n    \n    # ワンホットエンコーディング対象のカテゴリカル変数\n    categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group']\n    \n    # ===== 6. 相互作用特徴量 =====\n    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\n    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\n    \n    interaction_cols = ['job_education', 'contact_month']\n    categorical_cols.extend(interaction_cols)\n    \n    # monthは既に周期性エンコーディングしたので削除\n    df = df.drop(columns=['month', 'month_numeric'])\n    \n    return df, categorical_cols\n\n# 特徴量エンジニアリングを適用\ntrain_processed, categorical_cols = feature_engineering(train_balanced, is_train=True)\ntest_processed, _ = feature_engineering(test_df, is_train=False)\n\nprint(\"特徴量エンジニアリング完了\")\nprint(f\"Train shape: {train_processed.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ワンホットエンコーディング実行\ntrain_encoded = pd.get_dummies(train_processed, columns=categorical_cols, drop_first=True)\ntest_encoded = pd.get_dummies(test_processed, columns=categorical_cols, drop_first=True)\n\n# LightGBMのためにすべてのカラム名から特殊文字を除去\ndef sanitize_column_names(df):\n    \"\"\"LightGBM用にカラム名から特殊JSON文字を除去\"\"\"\n    df.columns = df.columns.str.replace(r'[\\(\\)\\[\\]\\{\\}\\:\\\"\\,\\.\\s]', '_', regex=True)\n    # 連続するアンダースコアを1つに\n    df.columns = df.columns.str.replace(r'_+', '_', regex=True)\n    # 先頭と末尾のアンダースコアを除去\n    df.columns = df.columns.str.strip('_')\n    return df\n\ntrain_encoded = sanitize_column_names(train_encoded)\ntest_encoded = sanitize_column_names(test_encoded)\n\n# 訓練データとテストデータのカラムを揃える\nmissing_cols = set(train_encoded.columns) - set(test_encoded.columns)\nfor col in missing_cols:\n    if col != 'y':\n        test_encoded[col] = 0\n\nextra_cols = set(test_encoded.columns) - set(train_encoded.columns)\ntest_encoded = test_encoded.drop(columns=list(extra_cols))\n\n# カラムの順序を揃える\ntest_encoded = test_encoded[train_encoded.drop(columns=['y']).columns]\n\nprint(f\"ワンホットエンコーディング後のTrain shape: {train_encoded.shape}\")\nprint(f\"ワンホットエンコーディング後のTest shape: {test_encoded.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットと特徴量の分離\n",
    "y = train_encoded['y']\n",
    "X = train_encoded.drop(columns=['id', 'y'])\n",
    "X_test = test_encoded.drop(columns=['id'])\n",
    "\n",
    "# Train/Valid分割（バランスを保つためstratify使用）\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Valid set: {X_valid.shape}\")\n",
    "print(f\"\\nTrain set 正例率: {y_train.mean():.4f}\")\n",
    "print(f\"Valid set 正例率: {y_valid.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBMのハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 2000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return auc\n",
    "\n",
    "# 最適化実行\n",
    "print(\"ハイパーパラメータ最適化を開始します...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_balanced\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\nBest params:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適パラメータで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適パラメータで学習\n",
    "best_params = study_lgb.best_params.copy()\n",
    "best_params.update({\n",
    "    \"n_estimators\": 2000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "})\n",
    "\n",
    "model_lgb = lgb.LGBMClassifier(**best_params)\n",
    "model_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    callbacks=[lgb.early_stopping(100, verbose=True)]\n",
    ")\n",
    "\n",
    "# 検証データで評価\n",
    "pred_proba = model_lgb.predict_proba(X_valid)[:, 1]\n",
    "auc = roc_auc_score(y_valid, pred_proba)\n",
    "\n",
    "print(f\"\\nValidation AUC: {auc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適閾値の探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適な閾値を探索\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.3, 0.8, 0.01):\n",
    "    pred_binary = (pred_proba > threshold).astype(int)\n",
    "    f1 = f1_score(y_valid, pred_binary)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"最適閾値: {best_threshold:.3f}\")\n",
    "print(f\"最適F1スコア: {best_f1:.5f}\")\n",
    "\n",
    "# 最適閾値での評価\n",
    "pred_binary = (pred_proba > best_threshold).astype(int)\n",
    "print(f\"\\n=== 最適閾値での評価 ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, pred_binary):.5f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_valid, pred_binary))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の可視化\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'][:20], feature_importance['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 重要な特徴量:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータ予測と提出ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで予測\n",
    "test_pred_proba = model_lgb.predict_proba(X_test)[:, 1]\n",
    "test_pred_binary = (test_pred_proba > best_threshold).astype(int)\n",
    "\n",
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/user/bank/data/balanced_dataset_submission.csv', index=False, header=False)\n",
    "\n",
    "print(\"提出ファイルを作成しました: balanced_dataset_submission.csv\")\n",
    "print(f\"\\n予測分布:\")\n",
    "print(submission['y'].value_counts())\n",
    "print(f\"\\nPositive予測率: {submission['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率値も保存（閾値調整用）\n",
    "submission_proba = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y_proba': test_pred_proba,\n",
    "    'y_pred': test_pred_binary\n",
    "})\n",
    "\n",
    "submission_proba.to_csv('/home/user/bank/data/balanced_dataset_submission_with_proba.csv', index=False)\n",
    "print(\"確率値付き提出ファイルも作成しました: balanced_dataset_submission_with_proba.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}