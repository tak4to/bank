{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 高精度アンサンブルモデル: LightGBM + XGBoost + CatBoost + TabNet\\n",
    "\\n",
    "このノートブックでは、複数の強力なモデルを組み合わせて高精度な予測を実現します。\\n",
    "\\n",
    "## モデル構成\\n",
    "1. **LightGBM**: 高速で効率的な勾配ブースティング\\n",
    "2. **XGBoost**: 正則化が強力な勾配ブースティング\\n",
    "3. **CatBoost**: カテゴリカル特徴量に強い勾配ブースティング\\n",
    "4. **TabNet**: 注意機構を使った深層学習モデル\\n",
    "5. **Stacking**: 上記モデルの予測を統合するメタラーナー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "from sklearn.preprocessing import StandardScaler\\n",
    "from sklearn.model_selection import StratifiedKFold\\n",
    "from sklearn.metrics import (\\n",
    "    roc_auc_score, \\n",
    "    f1_score, \\n",
    "    accuracy_score,\\n",
    "    confusion_matrix,\\n",
    "    classification_report\\n",
    ")\\n",
    "from sklearn.linear_model import LogisticRegression\\n",
    "import optuna\\n",
    "import lightgbm as lgb\\n",
    "import xgboost as xgb\\n",
    "from catboost import CatBoostClassifier, Pool\\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\\n",
    "import torch\\n",
    "import warnings\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# 再現性のためのシード設定\\n",
    "RANDOM_STATE = 42\\n",
    "np.random.seed(RANDOM_STATE)\\n",
    "torch.manual_seed(RANDOM_STATE)\\n",
    "\\n",
    "print(\\"ライブラリのインポート完了\\")\\n",
    "print(f\\"PyTorch version: {torch.__version__}\\")\\n",
    "print(f\\"CUDA available: {torch.cuda.is_available()}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\\n",
    "train_df = pd.read_csv(\\"/home/user/bank/data/train.csv\\")\\n",
    "test_df = pd.read_csv(\\"/home/user/bank/data/test.csv\\")\\n",
    "\\n",
    "print(f\\"Train shape: {train_df.shape}\\")\\n",
    "print(f\\"Test shape: {test_df.shape}\\")\\n",
    "print(f\\"\\\\nTarget distribution:\\")\\n",
    "print(train_df['y'].value_counts())\\n",
    "print(f\\"\\\\nPositive rate: {train_df['y'].mean():.4f}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_train=True):\\n",
    "    \\"\\"\\"\\n",
    "    特徴量エンジニアリング関数（ワンホットエンコーディング版）\\n",
    "    \\"\\"\\"\\n",
    "    df = df.copy()\\n",
    "    \\n",
    "    # ===== 1. 数値特徴量の変換 =====\\n",
    "    df['age_group'] = pd.cut(df['age'], bins=16).astype(str)\\n",
    "    df['age_group'] = df['age_group'].str.replace(r'[(),.\\\\ [\\\\\\\\] ]', '_', regex=True)\\n",
    "    \\n",
    "    # balance の対数変換\\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\\n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\\n",
    "    \\n",
    "    # ===== 2. 時系列特徴量 =====\\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\\n",
    "    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\\n",
    "    df['duration_log'] = np.log1p(df['duration'])\\n",
    "    \\n",
    "    # previous関連\\n",
    "    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\\n",
    "    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\\n",
    "    \\n",
    "    # ===== 3. 月のマッピングと周期性エンコーディング =====\\n",
    "    month_mapping = {\\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\\n",
    "    }\\n",
    "    df['month_numeric'] = df['month'].map(month_mapping)\\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\\n",
    "    \\n",
    "    # ===== 4. ローン関連の特徴量 =====\\n",
    "    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\\n",
    "    \\n",
    "    # ===== 5. カテゴリカル特徴量の準備 =====\\n",
    "    binary_cols = ['default', 'housing', 'loan']\\n",
    "    for col in binary_cols:\\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\\n",
    "    \\n",
    "    # ワンホットエンコーディング対象のカテゴリカル変数\\n",
    "    categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group']\\n",
    "    \\n",
    "    # ===== 6. 相互作用特徴量 =====\\n",
    "    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\\n",
    "    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\\n",
    "    \\n",
    "    interaction_cols = ['job_education', 'contact_month']\\n",
    "    categorical_cols.extend(interaction_cols)\\n",
    "    \\n",
    "    df = df.drop(columns=['month', 'month_numeric'])\\n",
    "    \\n",
    "    return df, categorical_cols\\n",
    "\\n",
    "# 特徴量エンジニアリングを適用\\n",
    "train_processed, categorical_cols = feature_engineering(train_df, is_train=True)\\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False)\\n",
    "\\n",
    "print(\\"特徴量エンジニアリング完了\\")\\n",
    "print(f\\"Train shape: {train_processed.shape}\\")\\n",
    "print(f\\"\\\\nカテゴリカル変数: {categorical_cols}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onehot_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワンホットエンコーディング実行\\n",
    "train_encoded = pd.get_dummies(train_processed, columns=categorical_cols, drop_first=True)\\n",
    "test_encoded = pd.get_dummies(test_processed, columns=categorical_cols, drop_first=True)\\n",
    "\\n",
    "# 訓練データとテストデータのカラムを揃える\\n",
    "missing_cols = set(train_encoded.columns) - set(test_encoded.columns)\\n",
    "for col in missing_cols:\\n",
    "    if col != 'y':\\n",
    "        test_encoded[col] = 0\\n",
    "\\n",
    "extra_cols = set(test_encoded.columns) - set(train_encoded.columns)\\n",
    "test_encoded = test_encoded.drop(columns=list(extra_cols))\\n",
    "\\n",
    "test_encoded = test_encoded[train_encoded.drop(columns=['y']).columns]\\n",
    "\\n",
    "print(f\\"ワンホットエンコーディング後のTrain shape: {train_encoded.shape}\\")\\n",
    "print(f\\"ワンホットエンコーディング後のTest shape: {test_encoded.shape}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットと特徴量の分離\\n",
    "y = train_encoded['y']\\n",
    "X = train_encoded.drop(columns=['id', 'y'])\\n",
    "X_test = test_encoded.drop(columns=['id'])\\n",
    "\\n",
    "print(f\\"特徴量数: {X.shape[1]}\\")\\n",
    "print(f\\"訓練データサンプル数: {X.shape[0]}\\")\\n",
    "print(f\\"テストデータサンプル数: {X_test.shape[0]}\\")\\n",
    "\\n",
    "# 交差検証の設定\\n",
    "N_SPLITS = 5\\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model1",
   "metadata": {},
   "source": [
    "## 1. LightGBMモデルの最適化と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgb_optimize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\\n",
    "    \\"\\"\\"LightGBMのハイパーパラメータ最適化\\"\\"\\"\\n",
    "    params = {\\n",
    "        \\"learning_rate\\": trial.suggest_float(\\"learning_rate\\", 0.01, 0.1, log=True),\\n",
    "        \\"num_leaves\\": trial.suggest_int(\\"num_leaves\\", 20, 150),\\n",
    "        \\"max_depth\\": trial.suggest_int(\\"max_depth\\", 3, 12),\\n",
    "        \\"min_child_samples\\": trial.suggest_int(\\"min_child_samples\\", 10, 100),\\n",
    "        \\"subsample\\": trial.suggest_float(\\"subsample\\", 0.6, 1.0),\\n",
    "        \\"colsample_bytree\\": trial.suggest_float(\\"colsample_bytree\\", 0.6, 1.0),\\n",
    "        \\"reg_alpha\\": trial.suggest_float(\\"reg_alpha\\", 1e-8, 10.0, log=True),\\n",
    "        \\"reg_lambda\\": trial.suggest_float(\\"reg_lambda\\", 1e-8, 10.0, log=True),\\n",
    "        \\"min_split_gain\\": trial.suggest_float(\\"min_split_gain\\", 0.0, 1.0),\\n",
    "        \\"n_estimators\\": 3000,\\n",
    "        \\"objective\\": \\"binary\\",\\n",
    "        \\"metric\\": \\"auc\\",\\n",
    "        \\"verbosity\\": -1,\\n",
    "        \\"random_state\\": RANDOM_STATE,\\n",
    "        \\"class_weight\\": \\"balanced\\"\\n",
    "    }\\n",
    "    \\n",
    "    cv_scores = []\\n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "        \\n",
    "        model = lgb.LGBMClassifier(**params)\\n",
    "        model.fit(\\n",
    "            X_train_fold, y_train_fold,\\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\\n",
    "            callbacks=[\\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\\n",
    "                lgb.log_evaluation(period=0)\\n",
    "            ]\\n",
    "        )\\n",
    "        \\n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\\n",
    "        cv_scores.append(auc)\\n",
    "    \\n",
    "    return np.mean(cv_scores)\\n",
    "\\n",
    "print(\\"LightGBMのハイパーパラメータ最適化を開始...\\")\\n",
    "study_lgb = optuna.create_study(direction=\\"maximize\\", study_name=\\"lgbm\\")\\n",
    "study_lgb.optimize(objective_lgb, n_trials=30, show_progress_bar=True)\\n",
    "\\n",
    "print(f\\"\\\\nBest LightGBM CV AUC: {study_lgb.best_value:.5f}\\")\\n",
    "print(\\"Best params:\\")\\n",
    "for key, value in study_lgb.best_params.items():\\n",
    "    print(f\\"  {key}: {value}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lgb_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで交差検証学習\\n",
    "best_params_lgb = study_lgb.best_params.copy()\\n",
    "best_params_lgb.update({\\n",
    "    \\"n_estimators\\": 3000,\\n",
    "    \\"objective\\": \\"binary\\",\\n",
    "    \\"metric\\": \\"auc\\",\\n",
    "    \\"verbosity\\": -1,\\n",
    "    \\"random_state\\": RANDOM_STATE,\\n",
    "    \\"class_weight\\": \\"balanced\\"\\n",
    "})\\n",
    "\\n",
    "oof_lgb = np.zeros(len(X))\\n",
    "test_pred_lgb = np.zeros(len(X_test))\\n",
    "models_lgb = []\\n",
    "\\n",
    "print(\\"LightGBMで交差検証学習を開始...\\\\n\\")\\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "    print(f\\"Fold {fold + 1}/{N_SPLITS}\\")\\n",
    "    \\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "    \\n",
    "    model = lgb.LGBMClassifier(**best_params_lgb)\\n",
    "    model.fit(\\n",
    "        X_train_fold, y_train_fold,\\n",
    "        eval_set=[(X_valid_fold, y_valid_fold)],\\n",
    "        callbacks=[\\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\\n",
    "            lgb.log_evaluation(period=0)\\n",
    "        ]\\n",
    "    )\\n",
    "    \\n",
    "    oof_lgb[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "    test_pred_lgb += model.predict_proba(X_test)[:, 1] / N_SPLITS\\n",
    "    models_lgb.append(model)\\n",
    "    \\n",
    "    fold_auc = roc_auc_score(y_valid_fold, oof_lgb[valid_idx])\\n",
    "    print(f\\"  Fold {fold + 1} AUC: {fold_auc:.5f}\\\\n\\")\\n",
    "\\n",
    "lgb_auc = roc_auc_score(y, oof_lgb)\\n",
    "print(f\\"LightGBM OOF AUC: {lgb_auc:.5f}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model2",
   "metadata": {},
   "source": [
    "## 2. XGBoostモデルの最適化と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_optimize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\\n",
    "    \\"\\"\\"XGBoostのハイパーパラメータ最適化\\"\\"\\"\\n",
    "    params = {\\n",
    "        \\"learning_rate\\": trial.suggest_float(\\"learning_rate\\", 0.01, 0.1, log=True),\\n",
    "        \\"max_depth\\": trial.suggest_int(\\"max_depth\\", 3, 12),\\n",
    "        \\"min_child_weight\\": trial.suggest_int(\\"min_child_weight\\", 1, 10),\\n",
    "        \\"subsample\\": trial.suggest_float(\\"subsample\\", 0.6, 1.0),\\n",
    "        \\"colsample_bytree\\": trial.suggest_float(\\"colsample_bytree\\", 0.6, 1.0),\\n",
    "        \\"gamma\\": trial.suggest_float(\\"gamma\\", 0.0, 1.0),\\n",
    "        \\"reg_alpha\\": trial.suggest_float(\\"reg_alpha\\", 1e-8, 10.0, log=True),\\n",
    "        \\"reg_lambda\\": trial.suggest_float(\\"reg_lambda\\", 1e-8, 10.0, log=True),\\n",
    "        \\"n_estimators\\": 3000,\\n",
    "        \\"objective\\": \\"binary:logistic\\",\\n",
    "        \\"eval_metric\\": \\"auc\\",\\n",
    "        \\"random_state\\": RANDOM_STATE,\\n",
    "        \\"tree_method\\": \\"hist\\",\\n",
    "        \\"verbosity\\": 0\\n",
    "    }\\n",
    "    \\n",
    "    # スケール調整（不均衡データ対応）\\n",
    "    scale_pos_weight = (y == 0).sum() / (y == 1).sum()\\n",
    "    params[\\"scale_pos_weight\\"] = scale_pos_weight\\n",
    "    \\n",
    "    cv_scores = []\\n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "        \\n",
    "        model = xgb.XGBClassifier(**params)\\n",
    "        model.fit(\\n",
    "            X_train_fold, y_train_fold,\\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\\n",
    "            verbose=False\\n",
    "        )\\n",
    "        \\n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\\n",
    "        cv_scores.append(auc)\\n",
    "    \\n",
    "    return np.mean(cv_scores)\\n",
    "\\n",
    "print(\\"XGBoostのハイパーパラメータ最適化を開始...\\")\\n",
    "study_xgb = optuna.create_study(direction=\\"maximize\\", study_name=\\"xgboost\\")\\n",
    "study_xgb.optimize(objective_xgb, n_trials=30, show_progress_bar=True)\\n",
    "\\n",
    "print(f\\"\\\\nBest XGBoost CV AUC: {study_xgb.best_value:.5f}\\")\\n",
    "print(\\"Best params:\\")\\n",
    "for key, value in study_xgb.best_params.items():\\n",
    "    print(f\\"  {key}: {value}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostで交差検証学習\\n",
    "best_params_xgb = study_xgb.best_params.copy()\\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\\n",
    "best_params_xgb.update({\\n",
    "    \\"n_estimators\\": 3000,\\n",
    "    \\"objective\\": \\"binary:logistic\\",\\n",
    "    \\"eval_metric\\": \\"auc\\",\\n",
    "    \\"random_state\\": RANDOM_STATE,\\n",
    "    \\"tree_method\\": \\"hist\\",\\n",
    "    \\"scale_pos_weight\\": scale_pos_weight,\\n",
    "    \\"verbosity\\": 0\\n",
    "})\\n",
    "\\n",
    "oof_xgb = np.zeros(len(X))\\n",
    "test_pred_xgb = np.zeros(len(X_test))\\n",
    "models_xgb = []\\n",
    "\\n",
    "print(\\"XGBoostで交差検証学習を開始...\\\\n\\")\\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "    print(f\\"Fold {fold + 1}/{N_SPLITS}\\")\\n",
    "    \\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "    \\n",
    "    model = xgb.XGBClassifier(**best_params_xgb)\\n",
    "    model.fit(\\n",
    "        X_train_fold, y_train_fold,\\n",
    "        eval_set=[(X_valid_fold, y_valid_fold)],\\n",
    "        verbose=False\\n",
    "    )\\n",
    "    \\n",
    "    oof_xgb[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "    test_pred_xgb += model.predict_proba(X_test)[:, 1] / N_SPLITS\\n",
    "    models_xgb.append(model)\\n",
    "    \\n",
    "    fold_auc = roc_auc_score(y_valid_fold, oof_xgb[valid_idx])\\n",
    "    print(f\\"  Fold {fold + 1} AUC: {fold_auc:.5f}\\\\n\\")\\n",
    "\\n",
    "xgb_auc = roc_auc_score(y, oof_xgb)\\n",
    "print(f\\"XGBoost OOF AUC: {xgb_auc:.5f}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model3",
   "metadata": {},
   "source": [
    "## 3. CatBoostモデルの最適化と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cat_optimize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cat(trial):\\n",
    "    \\"\\"\\"CatBoostのハイパーパラメータ最適化\\"\\"\\"\\n",
    "    params = {\\n",
    "        \\"learning_rate\\": trial.suggest_float(\\"learning_rate\\", 0.01, 0.1, log=True),\\n",
    "        \\"depth\\": trial.suggest_int(\\"depth\\", 3, 10),\\n",
    "        \\"l2_leaf_reg\\": trial.suggest_float(\\"l2_leaf_reg\\", 1e-8, 10.0, log=True),\\n",
    "        \\"bagging_temperature\\": trial.suggest_float(\\"bagging_temperature\\", 0.0, 1.0),\\n",
    "        \\"random_strength\\": trial.suggest_float(\\"random_strength\\", 0.0, 10.0),\\n",
    "        \\"iterations\\": 3000,\\n",
    "        \\"eval_metric\\": \\"AUC\\",\\n",
    "        \\"random_seed\\": RANDOM_STATE,\\n",
    "        \\"verbose\\": False,\\n",
    "        \\"early_stopping_rounds\\": 100,\\n",
    "        \\"auto_class_weights\\": \\"Balanced\\"\\n",
    "    }\\n",
    "    \\n",
    "    cv_scores = []\\n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "        \\n",
    "        train_pool = Pool(X_train_fold, y_train_fold)\\n",
    "        valid_pool = Pool(X_valid_fold, y_valid_fold)\\n",
    "        \\n",
    "        model = CatBoostClassifier(**params)\\n",
    "        model.fit(train_pool, eval_set=valid_pool)\\n",
    "        \\n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\\n",
    "        cv_scores.append(auc)\\n",
    "    \\n",
    "    return np.mean(cv_scores)\\n",
    "\\n",
    "print(\\"CatBoostのハイパーパラメータ最適化を開始...\\")\\n",
    "study_cat = optuna.create_study(direction=\\"maximize\\", study_name=\\"catboost\\")\\n",
    "study_cat.optimize(objective_cat, n_trials=30, show_progress_bar=True)\\n",
    "\\n",
    "print(f\\"\\\\nBest CatBoost CV AUC: {study_cat.best_value:.5f}\\")\\n",
    "print(\\"Best params:\\")\\n",
    "for key, value in study_cat.best_params.items():\\n",
    "    print(f\\"  {key}: {value}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cat_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostで交差検証学習\\n",
    "best_params_cat = study_cat.best_params.copy()\\n",
    "best_params_cat.update({\\n",
    "    \\"iterations\\": 3000,\\n",
    "    \\"eval_metric\\": \\"AUC\\",\\n",
    "    \\"random_seed\\": RANDOM_STATE,\\n",
    "    \\"verbose\\": False,\\n",
    "    \\"early_stopping_rounds\\": 100,\\n",
    "    \\"auto_class_weights\\": \\"Balanced\\"\\n",
    "})\\n",
    "\\n",
    "oof_cat = np.zeros(len(X))\\n",
    "test_pred_cat = np.zeros(len(X_test))\\n",
    "models_cat = []\\n",
    "\\n",
    "print(\\"CatBoostで交差検証学習を開始...\\\\n\\")\\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "    print(f\\"Fold {fold + 1}/{N_SPLITS}\\")\\n",
    "    \\n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\\n",
    "    \\n",
    "    train_pool = Pool(X_train_fold, y_train_fold)\\n",
    "    valid_pool = Pool(X_valid_fold, y_valid_fold)\\n",
    "    \\n",
    "    model = CatBoostClassifier(**best_params_cat)\\n",
    "    model.fit(train_pool, eval_set=valid_pool)\\n",
    "    \\n",
    "    oof_cat[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "    test_pred_cat += model.predict_proba(X_test)[:, 1] / N_SPLITS\\n",
    "    models_cat.append(model)\\n",
    "    \\n",
    "    fold_auc = roc_auc_score(y_valid_fold, oof_cat[valid_idx])\\n",
    "    print(f\\"  Fold {fold + 1} AUC: {fold_auc:.5f}\\\\n\\")\\n",
    "\\n",
    "cat_auc = roc_auc_score(y, oof_cat)\\n",
    "print(f\\"CatBoost OOF AUC: {cat_auc:.5f}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model4",
   "metadata": {},
   "source": [
    "## 4. TabNetモデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tabnet_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNetで交差検証学習\\n",
    "# TabNetは最適化に時間がかかるため、良いデフォルト設定を使用\\n",
    "\\n",
    "# データの標準化（TabNetには推奨）\\n",
    "scaler = StandardScaler()\\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\\n",
    "\\n",
    "oof_tabnet = np.zeros(len(X))\\n",
    "test_pred_tabnet = np.zeros(len(X_test))\\n",
    "models_tabnet = []\\n",
    "\\n",
    "print(\\"TabNetで交差検証学習を開始...\\\\n\\")\\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\\n",
    "    print(f\\"Fold {fold + 1}/{N_SPLITS}\\")\\n",
    "    \\n",
    "    X_train_fold = X_scaled.iloc[train_idx].values\\n",
    "    X_valid_fold = X_scaled.iloc[valid_idx].values\\n",
    "    y_train_fold = y.iloc[train_idx].values.reshape(-1, 1)\\n",
    "    y_valid_fold = y.iloc[valid_idx].values.reshape(-1, 1)\\n",
    "    \\n",
    "    model = TabNetClassifier(\\n",
    "        n_d=32,\\n",
    "        n_a=32,\\n",
    "        n_steps=5,\\n",
    "        gamma=1.5,\\n",
    "        n_independent=2,\\n",
    "        n_shared=2,\\n",
    "        lambda_sparse=1e-4,\\n",
    "        optimizer_fn=torch.optim.Adam,\\n",
    "        optimizer_params=dict(lr=2e-2),\\n",
    "        scheduler_params={\\"step_size\\": 50, \\"gamma\\": 0.9},\\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\\n",
    "        mask_type='entmax',\\n",
    "        seed=RANDOM_STATE,\\n",
    "        verbose=0\\n",
    "    )\\n",
    "    \\n",
    "    model.fit(\\n",
    "        X_train_fold, y_train_fold,\\n",
    "        eval_set=[(X_valid_fold, y_valid_fold)],\\n",
    "        eval_metric=['auc'],\\n",
    "        max_epochs=200,\\n",
    "        patience=20,\\n",
    "        batch_size=256,\\n",
    "        virtual_batch_size=128\\n",
    "    )\\n",
    "    \\n",
    "    oof_tabnet[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\\n",
    "    test_pred_tabnet += model.predict_proba(X_test_scaled.values)[:, 1] / N_SPLITS\\n",
    "    models_tabnet.append(model)\\n",
    "    \\n",
    "    fold_auc = roc_auc_score(y.iloc[valid_idx], oof_tabnet[valid_idx])\\n",
    "    print(f\\"  Fold {fold + 1} AUC: {fold_auc:.5f}\\\\n\\")\\n",
    "\\n",
    "tabnet_auc = roc_auc_score(y, oof_tabnet)\\n",
    "print(f\\"TabNet OOF AUC: {tabnet_auc:.5f}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stacking",
   "metadata": {},
   "source": [
    "## 5. スタッキングアンサンブル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルのOOFスコアを比較\\n",
    "print(\\"\\\\n\" + \\"=\\" * 60)\\n",
    "print(\\"各モデルのOOF AUCスコア\\")\\n",
    "print(\\"=\\" * 60)\\n",
    "print(f\\"LightGBM: {lgb_auc:.5f}\\")\\n",
    "print(f\\"XGBoost:  {xgb_auc:.5f}\\")\\n",
    "print(f\\"CatBoost: {cat_auc:.5f}\\")\\n",
    "print(f\\"TabNet:   {tabnet_auc:.5f}\\")\\n",
    "print(\\"=\\" * 60)\\n",
    "\\n",
    "# 単純平均アンサンブル\\n",
    "oof_avg = (oof_lgb + oof_xgb + oof_cat + oof_tabnet) / 4\\n",
    "test_pred_avg = (test_pred_lgb + test_pred_xgb + test_pred_cat + test_pred_tabnet) / 4\\n",
    "avg_auc = roc_auc_score(y, oof_avg)\\n",
    "print(f\\"\\\\n単純平均アンサンブル OOF AUC: {avg_auc:.5f}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking_meta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタモデルでスタッキング（ロジスティック回帰）\\n",
    "meta_features = np.column_stack([oof_lgb, oof_xgb, oof_cat, oof_tabnet])\\n",
    "meta_test = np.column_stack([test_pred_lgb, test_pred_xgb, test_pred_cat, test_pred_tabnet])\\n",
    "\\n",
    "# メタモデルの学習\\n",
    "meta_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\\n",
    "meta_model.fit(meta_features, y)\\n",
    "\\n",
    "# メタモデルでの予測\\n",
    "oof_stacking = meta_model.predict_proba(meta_features)[:, 1]\\n",
    "test_pred_stacking = meta_model.predict_proba(meta_test)[:, 1]\\n",
    "stacking_auc = roc_auc_score(y, oof_stacking)\\n",
    "\\n",
    "print(f\\"スタッキングアンサンブル OOF AUC: {stacking_auc:.5f}\\")\\n",
    "print(f\\"\\\\nメタモデルの重み:\\")\\n",
    "print(f\\"  LightGBM: {meta_model.coef_[0][0]:.4f}\\")\\n",
    "print(f\\"  XGBoost:  {meta_model.coef_[0][1]:.4f}\\")\\n",
    "print(f\\"  CatBoost: {meta_model.coef_[0][2]:.4f}\\")\\n",
    "print(f\\"  TabNet:   {meta_model.coef_[0][3]:.4f}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良モデルの選択\\n",
    "all_scores = {\\n",
    "    'LightGBM': (lgb_auc, oof_lgb, test_pred_lgb),\\n",
    "    'XGBoost': (xgb_auc, oof_xgb, test_pred_xgb),\\n",
    "    'CatBoost': (cat_auc, oof_cat, test_pred_cat),\\n",
    "    'TabNet': (tabnet_auc, oof_tabnet, test_pred_tabnet),\\n",
    "    'Average': (avg_auc, oof_avg, test_pred_avg),\\n",
    "    'Stacking': (stacking_auc, oof_stacking, test_pred_stacking)\\n",
    "}\\n",
    "\\n",
    "best_model_name = max(all_scores, key=lambda k: all_scores[k][0])\\n",
    "best_score, best_oof, best_test_pred = all_scores[best_model_name]\\n",
    "\\n",
    "print(f\\"\\\\n\\\\n{'='*60}\\")\\n",
    "print(f\\"最良モデル: {best_model_name}\\")\\n",
    "print(f\\"最良OOF AUC: {best_score:.5f}\\")\\n",
    "print(f\\"{'='*60}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval",
   "metadata": {},
   "source": [
    "## 6. 最終評価と提出ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold_opt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適閾値の探索\\n",
    "best_threshold = 0.5\\n",
    "best_f1 = 0\\n",
    "\\n",
    "for threshold in np.arange(0.3, 0.8, 0.01):\\n",
    "    pred_binary = (best_oof > threshold).astype(int)\\n",
    "    f1 = f1_score(y, pred_binary)\\n",
    "    if f1 > best_f1:\\n",
    "        best_f1 = f1\\n",
    "        best_threshold = threshold\\n",
    "\\n",
    "print(f\\"最適閾値: {best_threshold:.3f}\\")\\n",
    "print(f\\"最適F1スコア: {best_f1:.5f}\\")\\n",
    "\\n",
    "# 最適閾値での評価\\n",
    "oof_binary = (best_oof > best_threshold).astype(int)\\n",
    "print(f\\"\\\\n=== 最適閾値での評価 ===\\")\\n",
    "print(f\\"Accuracy: {accuracy_score(y, oof_binary):.5f}\\")\\n",
    "print(f\\"\\\\nConfusion Matrix:\\")\\n",
    "print(confusion_matrix(y, oof_binary))\\n",
    "print(f\\"\\\\nClassification Report:\\")\\n",
    "print(classification_report(y, oof_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出ファイル作成\\n",
    "test_pred_binary = (best_test_pred > best_threshold).astype(int)\\n",
    "\\n",
    "submission = pd.DataFrame({\\n",
    "    'id': test_df['id'],\\n",
    "    'y': test_pred_binary\\n",
    "})\\n",
    "\\n",
    "submission.to_csv('/home/user/bank/data/deep_learning_ensemble_submission.csv', index=False, header=False)\\n",
    "\\n",
    "print(\\"提出ファイルを作成しました: deep_learning_ensemble_submission.csv\\")\\n",
    "print(f\\"\\\\n予測分布:\\")\\n",
    "print(submission['y'].value_counts())\\n",
    "print(f\\"\\\\nPositive予測率: {submission['y'].mean():.4f}\\")\\n",
    "\\n",
    "# 確率値も保存\\n",
    "submission_proba = pd.DataFrame({\\n",
    "    'id': test_df['id'],\\n",
    "    'y_proba': best_test_pred,\\n",
    "    'y_pred': test_pred_binary\\n",
    "})\\n",
    "\\n",
    "submission_proba.to_csv('/home/user/bank/data/deep_learning_ensemble_submission_with_proba.csv', index=False)\\n",
    "print(\\"\\\\n確率値付き提出ファイルも作成しました: deep_learning_ensemble_submission_with_proba.csv\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の可視化（LightGBMモデルを使用）\\n",
    "feature_importance = pd.DataFrame({\\n",
    "    'feature': X.columns,\\n",
    "    'importance': models_lgb[-1].feature_importances_\\n",
    "}).sort_values('importance', ascending=False)\\n",
    "\\n",
    "plt.figure(figsize=(10, 8))\\n",
    "plt.barh(feature_importance['feature'][:20], feature_importance['importance'][:20])\\n",
    "plt.xlabel('Importance')\\n",
    "plt.title('Top 20 Feature Importances (LightGBM)')\\n",
    "plt.gca().invert_yaxis()\\n",
    "plt.tight_layout()\\n",
    "plt.show()\\n",
    "\\n",
    "print(\\"\\\\nTop 20 重要な特徴量:\\")\\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル間の予測相関を確認\\n",
    "pred_correlation = pd.DataFrame({\\n",
    "    'LightGBM': oof_lgb,\\n",
    "    'XGBoost': oof_xgb,\\n",
    "    'CatBoost': oof_cat,\\n",
    "    'TabNet': oof_tabnet\\n",
    "}).corr()\\n",
    "\\n",
    "plt.figure(figsize=(8, 6))\\n",
    "sns.heatmap(pred_correlation, annot=True, cmap='coolwarm', center=0, fmt='.3f')\\n",
    "plt.title('モデル間の予測相関')\\n",
    "plt.tight_layout()\\n",
    "plt.show()\\n",
    "\\n",
    "print(\\"\\\\nモデル間の予測相関:\\")\\n",
    "print(pred_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## まとめ\\n",
    "\\n",
    "このノートブックでは、以下の高精度モデルを実装しました：\\n",
    "\\n",
    "1. **LightGBM**: 高速で効率的な勾配ブースティング\\n",
    "2. **XGBoost**: 正則化が強力な勾配ブースティング\\n",
    "3. **CatBoost**: カテゴリカル特徴量に強い勾配ブースティング\\n",
    "4. **TabNet**: 注意機構を使った深層学習モデル\\n",
    "5. **スタッキングアンサンブル**: 全モデルの予測を統合\\n",
    "\\n",
    "各モデルはOptunaで最適化され、5-Fold交差検証で評価されました。\\n",
    "最終的な予測は、最も高いOOF AUCスコアを達成したモデルを使用しています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
