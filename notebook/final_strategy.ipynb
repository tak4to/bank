{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326d35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Starting Cross-Validation (5 Folds)...\n",
      "--------------------------------------------------\n",
      "Fold 1 AUC: 0.93396\n",
      "Fold 2 AUC: 0.92553\n",
      "Fold 3 AUC: 0.92482\n",
      "Fold 4 AUC: 0.93502\n",
      "Fold 5 AUC: 0.93824\n",
      "--------------------------------------------------\n",
      "Mean AUC: 0.93151\n",
      "OOF AUC : 0.93119\n",
      "\n",
      "[Adviser's Note]\n",
      "この Mean AUC があなたの以前のスコアを上回っていれば、私の理論が正しい証明です。\n",
      "テストデータ(test.csv)があるなら、このモデルで全学習データを使って再学習し、predict_probaで予測して提出してください。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 1. データ読み込み\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# 2. 特徴量エンジニアリング (余計なことはしない、重要なことだけやる)\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- 数値変数の処理 ---\n",
    "    # pdays: -1 は「過去の接触なし」。これを数値としてそのまま扱うと距離がおかしくなるが、\n",
    "    # ツリーモデルは分割できるのでそのままでも良い。\n",
    "    # しかし、明示的に「接触なしフラグ」を作ると親切。\n",
    "    df['pdays_never'] = (df['pdays'] == -1).astype(int)\n",
    "    \n",
    "    # duration: 対数変換などを試してもいいが、ツリーモデルはそのままでも非線形性を捉える。\n",
    "    # 重要なのは相互作用。\n",
    "    # 1回の接触あたりの平均時間のような指標\n",
    "    df['duration_per_contact'] = df['duration'] / (df['campaign'] + 1e-5)\n",
    "    \n",
    "    # balance: 負の値があるため、対数変換する場合は注意が必要だが、\n",
    "    # ここでは生の値のまま扱う（ツリーモデルはスケールに依存しないため）。\n",
    "    \n",
    "    # --- カテゴリ変数の処理 ---\n",
    "    # One-Hot Encodingはメモリを食う上に、ツリーの深さを無駄に消費する。\n",
    "    # Ordinal Encoding（数値への置き換え）で十分、あるいはTarget Encodingがベスト。\n",
    "    # ここではシンプルかつ強力なOrdinal Encodingを採用する。\n",
    "    # カテゴリカルとして扱うカラム\n",
    "    cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "    \n",
    "    # 欠損値や未知のカテゴリ('unknown')はそのままで意味を持つので、特別扱いしない。\n",
    "    \n",
    "    return df, cat_cols\n",
    "\n",
    "train_processed, cat_cols = preprocess(train_df)\n",
    "\n",
    "# カテゴリ変数を数値IDに変換 (Ordinal Encoding)\n",
    "# HistGradientBoostingClassifierはカテゴリ特徴量を指定すればネイティブに扱えるが、\n",
    "# 文字列のままではエラーになるため数値化する。\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "train_processed[cat_cols] = oe.fit_transform(train_processed[cat_cols])\n",
    "\n",
    "# 3. モデリング & 評価 (Stratified K-Fold)\n",
    "X = train_processed.drop(columns=['id', 'y'])\n",
    "y = train_processed['y']\n",
    "categorical_features_indices = [X.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X))\n",
    "scores = []\n",
    "\n",
    "print(\"\\nStarting Cross-Validation (5 Folds)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # モデル構築\n",
    "    # class_weightはあえて設定しない。AUCには不要なことが多い。\n",
    "    # どうしてもRecallを上げたい場合のみ検討すべき。\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        max_iter=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=10,\n",
    "        early_stopping=True,\n",
    "        categorical_features=categorical_features_indices,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 予測（確率値）\n",
    "    val_preds = model.predict_proba(X_val)[:, 1]\n",
    "    oof_preds[val_index] = val_preds\n",
    "    \n",
    "    score = roc_auc_score(y_val, val_preds)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {fold+1} AUC: {score:.5f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Mean AUC: {np.mean(scores):.5f}\")\n",
    "print(f\"OOF AUC : {roc_auc_score(y, oof_preds):.5f}\")\n",
    "\n",
    "print(\"\\n[Adviser's Note]\")\n",
    "print(\"この Mean AUC があなたの以前のスコアを上回っていれば、私の理論が正しい証明です。\")\n",
    "print(\"テストデータ(test.csv)があるなら、このモデルで全学習データを使って再学習し、predict_probaで予測して提出してください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1068d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (26128, 18), Test shape: (1000, 17)\n",
      "\n",
      "Starting Training...\n",
      "Fold 1 AUC: 0.92970\n",
      "Fold 2 AUC: 0.93088\n",
      "Fold 3 AUC: 0.92638\n",
      "Fold 4 AUC: 0.93212\n",
      "Fold 5 AUC: 0.93220\n",
      "------------------------------\n",
      "OOF AUC: 0.93020\n",
      "Submission file created (in memory).\n",
      "          id         y\n",
      "26128  30000  0.011198\n",
      "26129  30001  0.011907\n",
      "26130  30002  0.005742\n",
      "26131  30003  0.018738\n",
      "26132  30004  0.037102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# LightGBMが使えるならLightGBMを使ってください。ここでは標準ライブラリのHistGradientBoostingClassifierを使います。\n",
    "# 性能はほぼ同等です。\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# --- 1. データ読み込み ---\n",
    "# あなたの環境に合わせてパスを調整してください\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "# test = pd.read_csv('test.csv') \n",
    "# ※ここではテストデータがないため、trainの一部をtestとして擬似的に作成して動作させます\n",
    "# 本番では上の行のコメントアウトを外し、下の3行を削除してください\n",
    "test = train.sample(1000, random_state=42).drop(columns=['y']).copy()\n",
    "test['id'] = range(30000, 31000) # ダミーID\n",
    "train = train.drop(test.index, errors='ignore').reset_index(drop=True)\n",
    "\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "\n",
    "# データ結合（前処理を一括で行うため）\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['y'] = np.nan # テストデータのターゲットは不明\n",
    "\n",
    "# 結合\n",
    "df_all = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# --- 2. 高度な特徴量エンジニアリング ---\n",
    "\n",
    "# (A) 日付・季節性の処理\n",
    "month_map = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "df_all['month_int'] = df_all['month'].map(month_map)\n",
    "# 周期性（Monthの円環構造）\n",
    "df_all['month_sin'] = np.sin(2 * np.pi * df_all['month_int'] / 12)\n",
    "df_all['month_cos'] = np.cos(2 * np.pi * df_all['month_int'] / 12)\n",
    "\n",
    "# (B) Duration（最重要変数）の強化\n",
    "# 通話時間が0の場合は対数変換でエラーになるので+1\n",
    "df_all['duration_log'] = np.log1p(df_all['duration'])\n",
    "# キャンペーン回数あたりの通話時間（粘り強さ？）\n",
    "df_all['duration_per_campaign'] = df_all['duration'] / (df_all['campaign'] + 1e-5)\n",
    "\n",
    "# (C) 顧客プロファイルの擬似特定 (User Fingerprint)\n",
    "# これらが一致すれば「同一人物」である可能性が高い\n",
    "user_cols = ['age', 'job', 'marital', 'education', 'housing', 'loan']\n",
    "df_all['user_hash'] = df_all[user_cols].astype(str).sum(axis=1)\n",
    "# このユーザーがデータセットに何回登場するか？\n",
    "df_all['user_count'] = df_all.groupby('user_hash')['id'].transform('count')\n",
    "\n",
    "# (D) Aggregation Features (集約特徴量) - ここがスコアアップの肝です\n",
    "# 「その職業(job)の平均通話時間は？」、「その教育レベル(education)の平均残高は？」\n",
    "# これと自分の値との差分（Diff）や比率（Ratio）をとります。\n",
    "\n",
    "group_cols = ['job', 'education', 'marital', 'contact']\n",
    "target_cols = ['duration', 'balance', 'pdays']\n",
    "\n",
    "for g_col in group_cols:\n",
    "    for t_col in target_cols:\n",
    "        # 平均\n",
    "        mean_col = df_all.groupby(g_col)[t_col].transform('mean')\n",
    "        df_all[f'{t_col}_mean_by_{g_col}'] = mean_col\n",
    "        # 自分の値との比率（自分の通話時間は、同じ職業の平均より長いか？）\n",
    "        df_all[f'{t_col}_ratio_by_{g_col}'] = df_all[t_col] / (mean_col + 1e-5)\n",
    "\n",
    "# (E) pdays（経過日数）の特別な処理\n",
    "# -1 は「連絡なし」。これを数値として混ぜると分布が歪む。\n",
    "# 「初めての客か？」フラグを作成\n",
    "df_all['is_new_client'] = (df_all['pdays'] == -1).astype(int)\n",
    "\n",
    "# --- 3. カテゴリ変数のエンコーディング ---\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Ordinal Encoding (LightGBM/HistGradientBoostingはこれで十分)\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "df_all[cat_cols] = oe.fit_transform(df_all[cat_cols].astype(str))\n",
    "\n",
    "# --- 4. データの分割 ---\n",
    "train_processed = df_all[df_all['is_train'] == 1].drop(columns=['is_train', 'user_hash'])\n",
    "test_processed = df_all[df_all['is_train'] == 0].drop(columns=['is_train', 'y', 'user_hash'])\n",
    "\n",
    "X = train_processed.drop(columns=['id', 'y'])\n",
    "y = train_processed['y']\n",
    "X_test = test_processed.drop(columns=['id'])\n",
    "\n",
    "# カテゴリカル変数のインデックスを取得\n",
    "cat_indices = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n",
    "\n",
    "# --- 5. モデリング (Stratified K-Fold + HistGradientBoosting) ---\n",
    "# LightGBMを使用する場合は lgb.LGBMClassifier に書き換えてください\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds_accum = np.zeros(len(X_test))\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # モデル設定：0.99を目指すなら過学習スレスレまで攻める設定もありだが、\n",
    "    # まずは堅実にEarly Stoppingを入れる\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        max_iter=2000,\n",
    "        learning_rate=0.03, # ゆっくり学習させる\n",
    "        max_depth=12,      # 少し深めに\n",
    "        l2_regularization=1.0, # 過学習抑制\n",
    "        categorical_features=cat_indices,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # 予測\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    oof_preds[val_idx] = val_pred\n",
    "    \n",
    "    score = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"Fold {fold+1} AUC: {score:.5f}\")\n",
    "    \n",
    "    # テストデータへの予測（平均をとるため加算）\n",
    "    test_preds_accum += model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 最終スコア\n",
    "mean_auc = roc_auc_score(y, oof_preds)\n",
    "print(\"-\" * 30)\n",
    "print(f\"OOF AUC: {mean_auc:.5f}\")\n",
    "\n",
    "# --- 6. 提出ファイル作成 ---\n",
    "test_preds_avg = test_preds_accum / skf.get_n_splits()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_processed['id'],\n",
    "    'y': test_preds_avg\n",
    "})\n",
    "\n",
    "# submission.to_csv('submission_advanced.csv', index=False)\n",
    "print(\"Submission file created (in memory).\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c27268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Test data loaded. Shape: (18083, 17)\n",
      "Engineering features...\n",
      "\n",
      "Starting Hybrid Ensemble Training...\n",
      "GBDT Part Done. Current AUC (Half): 0.93237\n",
      "Hybrid Ensemble OOF AUC: 0.93386\n",
      "\n",
      "Executing Dynamic Pseudo-Labeling...\n",
      "Adding 904 positive (Top 5%) and 904 negative pseudo-labels.\n",
      "\n",
      "Submission file created: 'submission_final_strategy.csv'\n",
      "   id         y\n",
      "0   1  0.827964\n",
      "1   2  0.709314\n",
      "2   3  0.000506\n",
      "3   4  0.000643\n",
      "4   5  0.116926\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# 1. 設定 & データ読み込み\n",
    "TRAIN_PATH = '../data/train.csv'\n",
    "TEST_PATH = '../data/test.csv'  # テストデータのファイル名\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "# --- テストデータの読み込み ---\n",
    "try:\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print(f\"Test data loaded. Shape: {test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"【警告】test.csvが見つかりません。学習用データの一部を代用してシミュレーションします。\")\n",
    "    print(\"本番提出用には、必ず正しいtest.csvを用意してください。\")\n",
    "    test = train.sample(2000, random_state=99).drop(columns=['y']).copy()\n",
    "    test['id'] = range(30000, 32000)\n",
    "    train = train.drop(test.index, errors='ignore').reset_index(drop=True)\n",
    "\n",
    "# データ結合\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['y'] = np.nan\n",
    "df_all = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "\n",
    "# 2. 特徴量エンジニアリング \n",
    "print(\"Engineering features...\")\n",
    "\n",
    "# (A) 数値データの変換\n",
    "df_all['duration_log'] = np.log1p(df_all['duration'])\n",
    "# キャンペーン効率\n",
    "df_all['efficiency'] = df_all['duration'] / (df_all['campaign'] + 1e-5)\n",
    "\n",
    "# (B) 強力な比率特徴量\n",
    "# 「その職業・年齢層の中で、今回の通話時間は異常か？」\n",
    "# 異常に長い＝脈あり、のシグナルを強調\n",
    "df_all['age_bin'] = pd.cut(df_all['age'], bins=5, labels=False)\n",
    "group_cols = ['job', 'housing', 'age_bin']\n",
    "target_cols = ['duration']\n",
    "\n",
    "for g_col in group_cols:\n",
    "    for t_col in target_cols:\n",
    "        # 平均との比率\n",
    "        mean_val = df_all.groupby(g_col)[t_col].transform('mean')\n",
    "        df_all[f'{t_col}_ratio_{g_col}'] = df_all[t_col] / (mean_val + 1e-5)\n",
    "\n",
    "# (C) 周期性 (Day/Month)\n",
    "# 給料日（25日など）や月末月初の影響を捉える\n",
    "month_map = {'jan':1, 'feb':2, 'mar':3, 'apr':4, 'may':5, 'jun':6, \n",
    "             'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov':11, 'dec':12}\n",
    "df_all['month_int'] = df_all['month'].map(month_map)\n",
    "df_all['month_sin'] = np.sin(2 * np.pi * df_all['month_int'] / 12)\n",
    "df_all['month_cos'] = np.cos(2 * np.pi * df_all['month_int'] / 12)\n",
    "\n",
    "df_all['day_sin'] = np.sin(2 * np.pi * df_all['day'] / 31)\n",
    "df_all['day_cos'] = np.cos(2 * np.pi * df_all['day'] / 31)\n",
    "\n",
    "# (D) 過去の成功体験 (最強の予測因子)\n",
    "# poutcome='success' はほぼ勝ち確。これを明示的にフラグ化。\n",
    "df_all['is_success_prev'] = (df_all['poutcome'] == 'success').astype(int)\n",
    "df_all['pdays_clean'] = df_all['pdays'].replace(-1, 999)\n",
    "\n",
    "# (E) カテゴリカル変数の処理\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "df_all[cat_cols] = oe.fit_transform(df_all[cat_cols].astype(str))\n",
    "\n",
    "# データの再分割\n",
    "X = df_all[df_all['is_train'] == 1].drop(columns=['id', 'y', 'is_train'])\n",
    "y = df_all[df_all['is_train'] == 1]['y']\n",
    "X_test = df_all[df_all['is_train'] == 0].drop(columns=['id', 'y', 'is_train'])\n",
    "\n",
    "# カテゴリ変数のインデックス（HGBC用）\n",
    "cat_indices = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n",
    "\n",
    "# 3. Hybrid Ensemble (GBDT + ExtraTrees)\n",
    "# 異なるアルゴリズムを混ぜることでスコアを安定させる\n",
    "print(\"\\nStarting Hybrid Ensemble Training...\")\n",
    "\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "\n",
    "# --- Model 1: HistGradientBoosting (Main) ---\n",
    "# Seed Averaging\n",
    "GBM_SEEDS = [42, 2024]\n",
    "for seed in GBM_SEEDS:\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = HistGradientBoostingClassifier(\n",
    "            max_iter=1000, learning_rate=0.03, max_depth=8,\n",
    "            categorical_features=cat_indices, early_stopping=True, random_state=seed\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        oof_preds[val_idx] += model.predict_proba(X_val)[:, 1] / len(GBM_SEEDS) / 2 # /2は後で他モデルと足すため\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / 5 / len(GBM_SEEDS) / 2\n",
    "\n",
    "print(f\"GBDT Part Done. Current AUC (Half): {roc_auc_score(y, oof_preds*2):.5f}\")\n",
    "\n",
    "# --- Model 2: ExtraTreesClassifier (Diversity) ---\n",
    "# 欠損値埋めが必要。\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=99)\n",
    "et_oof = np.zeros(len(X))\n",
    "et_test = np.zeros(len(X_test))\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_imputed, y):\n",
    "    X_tr, X_val = X_imputed[train_idx], X_imputed[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # 決定木をランダムに深く作るモデル\n",
    "    model = ExtraTreesClassifier(n_estimators=500, max_depth=20, min_samples_leaf=10, n_jobs=-1, random_state=99)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    et_oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    et_test += model.predict_proba(X_test_imputed)[:, 1] / 5\n",
    "\n",
    "# アンサンブル (GBDT: 70%, ExtraTrees: 30% くらいが相場だが、ここでは等倍ブレンドで安定を狙う)\n",
    "final_oof = oof_preds * 2 * 0.7 + et_oof * 0.3\n",
    "final_test_preds = test_preds * 2 * 0.7 + et_test * 0.3\n",
    "\n",
    "print(f\"Hybrid Ensemble OOF AUC: {roc_auc_score(y, final_oof):.5f}\")\n",
    "\n",
    "# 4. Dynamic Pseudo-Labeling (強制注入)\n",
    "print(\"\\nExecuting Dynamic Pseudo-Labeling...\")\n",
    "\n",
    "# 上位5%と下位5%を強制的に教師データにする\n",
    "# これなら「自信があるデータがない」という事態を防げる\n",
    "n_top = int(len(X_test) * 0.05)\n",
    "n_bottom = int(len(X_test) * 0.05)\n",
    "\n",
    "# インデックスを取得\n",
    "sorted_indices = np.argsort(final_test_preds)\n",
    "top_idx = sorted_indices[-n_top:]     # 予測確率が高いインデックス\n",
    "bottom_idx = sorted_indices[:n_bottom] # 予測確率が低いインデックス\n",
    "\n",
    "X_pseudo_high = X_test.iloc[top_idx]\n",
    "y_pseudo_high = pd.Series(1, index=X_pseudo_high.index)\n",
    "\n",
    "X_pseudo_low = X_test.iloc[bottom_idx]\n",
    "y_pseudo_low = pd.Series(0, index=X_pseudo_low.index)\n",
    "\n",
    "print(f\"Adding {len(X_pseudo_high)} positive (Top 5%) and {len(X_pseudo_low)} negative pseudo-labels.\")\n",
    "\n",
    "# データ拡張\n",
    "X_augmented = pd.concat([X, X_pseudo_high, X_pseudo_low])\n",
    "y_augmented = pd.concat([y, y_pseudo_high, y_pseudo_low])\n",
    "\n",
    "# 最終学習 (GBDTで全データ学習)\n",
    "final_model = HistGradientBoostingClassifier(\n",
    "    max_iter=1500, learning_rate=0.03, max_depth=8,\n",
    "    categorical_features=cat_indices, early_stopping=False, random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_augmented, y_augmented)\n",
    "submission_preds = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 念のため、アンサンブル結果とブレンド（暴走防止）\n",
    "submission_preds = 0.6 * submission_preds + 0.4 * final_test_preds\n",
    "\n",
    "# 5. 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'y': submission_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_final_strategy.csv', index=False)\n",
    "print(\"\\nSubmission file created: 'submission_final_strategy.csv'\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate-bank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
