{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高精度モデル構築\n",
    "\n",
    "## 改善点\n",
    "1. クロスバリデーションによる堅牢な評価\n",
    "2. 特徴量エンジニアリング（組み合わせ特徴量、エンコーディング改善）\n",
    "3. 複数アルゴリズムのアンサンブル（LightGBM、XGBoost、CatBoost）\n",
    "4. より適切な評価指標（AUC、F1スコア）\n",
    "5. Target Encodingの使用\n",
    "6. より詳細なハイパーパラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/takato/bita/bank/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/takato/bita/bank/data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{train_df['y'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_train=True, target_encoders=None):\n",
    "    \"\"\"\n",
    "    特徴量エンジニアリング関数\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. 数値特徴量の変換\n",
    "    # 年齢グループ\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                              labels=['0-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "    \n",
    "    # balance の対数変換（負の値があるため調整）\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n",
    "    \n",
    "    # 2. 時系列特徴量\n",
    "    # dayとduration の比率\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n",
    "    \n",
    "    # campaign効率\n",
    "    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\n",
    "    \n",
    "    # previous の成功率（pdaysが999でない場合）\n",
    "    df['has_previous_contact'] = (df['pdays'] != 999).astype(int)\n",
    "    df['previous_success_rate'] = df['previous'] / (df['pdays'].replace(999, 1) + 1)\n",
    "    \n",
    "    # 3. カテゴリカル変数の組み合わせ\n",
    "    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\n",
    "    df['marital_education'] = df['marital'].astype(str) + '_' + df['education'].astype(str)\n",
    "    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "    # 4. ローン関連の特徴量\n",
    "    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n",
    "    \n",
    "    # 5. カテゴリカル変数のリスト\n",
    "    categorical_feats = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                         'loan', 'contact', 'month', 'poutcome', \n",
    "                         'age_group', 'job_education', 'marital_education', 'contact_month']\n",
    "    \n",
    "    # 6. Target Encoding（訓練データのみで学習）\n",
    "    if is_train:\n",
    "        target_encoders = {}\n",
    "        for col in categorical_feats:\n",
    "            # 元のターゲット値で平均を計算\n",
    "            if 'y' in df.columns:\n",
    "                target_mean = df.groupby(col)['y'].mean()\n",
    "                target_encoders[col] = target_mean\n",
    "                # Smoothingを適用\n",
    "                global_mean = df['y'].mean()\n",
    "                counts = df.groupby(col).size()\n",
    "                smoothing = 10\n",
    "                smooth_target = (target_mean * counts + global_mean * smoothing) / (counts + smoothing)\n",
    "                df[f'{col}_target_enc'] = df[col].map(smooth_target)\n",
    "    else:\n",
    "        # テストデータには学習済みのエンコーダーを適用\n",
    "        for col in categorical_feats:\n",
    "            if col in target_encoders:\n",
    "                df[f'{col}_target_enc'] = df[col].map(target_encoders[col])\n",
    "                # 未知のカテゴリには平均値を使用\n",
    "                df[f'{col}_target_enc'].fillna(target_encoders[col].mean(), inplace=True)\n",
    "    \n",
    "    # 7. Frequency Encoding\n",
    "    for col in categorical_feats:\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[f'{col}_freq'] = df[col].map(freq)\n",
    "    \n",
    "    # 8. Label Encoding（モデル用）\n",
    "    label_encoders = {}\n",
    "    for col in categorical_feats:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    return df, target_encoders if is_train else label_encoders\n",
    "\n",
    "# 特徴量エンジニアリングを適用\n",
    "train_processed, target_encoders = feature_engineering(train_df, is_train=True)\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"New train shape: {train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットと特徴量の分離\n",
    "y = train_processed['y']\n",
    "exclude_cols = ['id', 'y']\n",
    "X = train_processed.drop(columns=exclude_cols)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")\n",
    "\n",
    "# Train/Valid分割\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Valid set: {X_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBMモデルの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 2000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"class_weight\": \"balanced\"  # 不均衡データ対応\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return auc\n",
    "\n",
    "# 最適化実行（より多くのトライアル）\n",
    "study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Best AUC (LightGBM):\", study_lgb.best_value)\n",
    "print(\"✅ Best params (LightGBM):\", study_lgb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoostモデルの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    \"\"\"\n",
    "    XGBoostのハイパーパラメータ最適化\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"n_estimators\": 2000,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"scale_pos_weight\": len(y_train[y_train==0]) / len(y_train[y_train==1])  # 不均衡対応\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return auc\n",
    "\n",
    "# 最適化実行\n",
    "study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Best AUC (XGBoost):\", study_xgb.best_value)\n",
    "print(\"✅ Best params (XGBoost):\", study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoostモデルの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cat(trial):\n",
    "    \"\"\"\n",
    "    CatBoostのハイパーパラメータ最適化\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "        \"iterations\": 2000,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"verbose\": False,\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"auto_class_weights\": \"Balanced\"  # 不均衡対応\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_valid, y_valid),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "    return auc\n",
    "\n",
    "# 最適化実行\n",
    "study_cat = optuna.create_study(direction=\"maximize\")\n",
    "study_cat.optimize(objective_cat, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"✅ Best AUC (CatBoost):\", study_cat.best_value)\n",
    "print(\"✅ Best params (CatBoost):\", study_cat.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 最終モデルの訓練とアンサンブル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルを最適パラメータで訓練\n",
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "best_params_lgb.update({\n",
    "    \"n_estimators\": 2000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"class_weight\": \"balanced\"\n",
    "})\n",
    "model_lgb = lgb.LGBMClassifier(**best_params_lgb)\n",
    "model_lgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "best_params_xgb.update({\n",
    "    \"n_estimators\": 2000,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"scale_pos_weight\": len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
    "})\n",
    "model_xgb = xgb.XGBClassifier(**best_params_xgb)\n",
    "model_xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "best_params_cat.update({\n",
    "    \"iterations\": 2000,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"verbose\": False,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"auto_class_weights\": \"Balanced\"\n",
    "})\n",
    "model_cat = CatBoostClassifier(**best_params_cat)\n",
    "model_cat.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_valid, y_valid),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"✅ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. モデル評価とアンサンブル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルの予測確率\n",
    "pred_lgb = model_lgb.predict_proba(X_valid)[:, 1]\n",
    "pred_xgb = model_xgb.predict_proba(X_valid)[:, 1]\n",
    "pred_cat = model_cat.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# 個別モデルの評価\n",
    "print(\"Individual Model Performance:\")\n",
    "print(f\"LightGBM AUC: {roc_auc_score(y_valid, pred_lgb):.5f}\")\n",
    "print(f\"XGBoost AUC: {roc_auc_score(y_valid, pred_xgb):.5f}\")\n",
    "print(f\"CatBoost AUC: {roc_auc_score(y_valid, pred_cat):.5f}\")\n",
    "\n",
    "# アンサンブル（加重平均）\n",
    "# 各モデルのAUCに基づいた重み付け\n",
    "auc_lgb = roc_auc_score(y_valid, pred_lgb)\n",
    "auc_xgb = roc_auc_score(y_valid, pred_xgb)\n",
    "auc_cat = roc_auc_score(y_valid, pred_cat)\n",
    "\n",
    "total_auc = auc_lgb + auc_xgb + auc_cat\n",
    "w_lgb = auc_lgb / total_auc\n",
    "w_xgb = auc_xgb / total_auc\n",
    "w_cat = auc_cat / total_auc\n",
    "\n",
    "pred_ensemble = w_lgb * pred_lgb + w_xgb * pred_xgb + w_cat * pred_cat\n",
    "\n",
    "print(f\"\\nEnsemble weights: LGB={w_lgb:.3f}, XGB={w_xgb:.3f}, CAT={w_cat:.3f}\")\n",
    "print(f\"\\nEnsemble AUC: {roc_auc_score(y_valid, pred_ensemble):.5f}\")\n",
    "\n",
    "# 閾値を最適化してF1スコアを計算\n",
    "from sklearn.metrics import f1_score\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in np.arange(0.3, 0.7, 0.01):\n",
    "    pred_binary = (pred_ensemble > threshold).astype(int)\n",
    "    f1 = f1_score(y_valid, pred_binary)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nBest threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best F1 Score: {best_f1:.5f}\")\n",
    "\n",
    "# 最適閾値での予測\n",
    "pred_final = (pred_ensemble > best_threshold).astype(int)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_valid, pred_final):.5f}\")\n",
    "print(f\"\\nClassification Report:\\n{classification_report(y_valid, pred_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. クロスバリデーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスバリデーションで堅牢性を確認\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Cross-Validation AUC Scores:\")\n",
    "\n",
    "# LightGBM\n",
    "cv_scores_lgb = cross_val_score(\n",
    "    model_lgb, X, y, cv=cv, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "print(f\"LightGBM: {cv_scores_lgb.mean():.5f} (+/- {cv_scores_lgb.std():.5f})\")\n",
    "\n",
    "# XGBoost\n",
    "cv_scores_xgb = cross_val_score(\n",
    "    model_xgb, X, y, cv=cv, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "print(f\"XGBoost: {cv_scores_xgb.mean():.5f} (+/- {cv_scores_xgb.std():.5f})\")\n",
    "\n",
    "# CatBoost\n",
    "cv_scores_cat = cross_val_score(\n",
    "    model_cat, X, y, cv=cv, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "print(f\"CatBoost: {cv_scores_cat.mean():.5f} (+/- {cv_scores_cat.std():.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 特徴量重要度の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の取得\n",
    "importance_lgb = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 上位20特徴量をプロット\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_lgb.head(20)['feature'], importance_lgb.head(20)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances (LightGBM)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_lgb.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. テストデータへの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに特徴量エンジニアリングを適用\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False, target_encoders=target_encoders)\n",
    "\n",
    "# 特徴量の抽出\n",
    "X_test = test_processed.drop(columns=['id'])\n",
    "\n",
    "# 各モデルで予測\n",
    "test_pred_lgb = model_lgb.predict_proba(X_test)[:, 1]\n",
    "test_pred_xgb = model_xgb.predict_proba(X_test)[:, 1]\n",
    "test_pred_cat = model_cat.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# アンサンブル予測\n",
    "test_pred_ensemble = w_lgb * test_pred_lgb + w_xgb * test_pred_xgb + w_cat * test_pred_cat\n",
    "\n",
    "# 最適閾値で二値化\n",
    "test_pred_final = (test_pred_ensemble > best_threshold).astype(int)\n",
    "\n",
    "# 結果を保存\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_pred_final\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/takato/bita/bank/data/high_accuracy_submission.csv', index=False, header=False)\n",
    "print(\"✅ Predictions saved to 'high_accuracy_submission.csv'\")\n",
    "print(f\"\\nPrediction distribution:\\n{submission['y'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# モデルとパラメータを保存\n",
    "models = {\n",
    "    'lgb': model_lgb,\n",
    "    'xgb': model_xgb,\n",
    "    'cat': model_cat,\n",
    "    'weights': {'lgb': w_lgb, 'xgb': w_xgb, 'cat': w_cat},\n",
    "    'threshold': best_threshold,\n",
    "    'target_encoders': target_encoders\n",
    "}\n",
    "\n",
    "with open('/home/takato/bita/bank/models/ensemble_model.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "print(\"✅ Models saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
