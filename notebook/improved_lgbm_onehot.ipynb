{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改善版: ワンホットエンコーディング + 交差検証 + LightGBM最適化\n",
    "\n",
    "このノートブックでは以下の改善を実装します:\n",
    "1. カテゴリ特徴量をワンホットエンコーディングで変換\n",
    "2. StratifiedKFold交差検証で汎化性能を向上\n",
    "3. LightGBMのハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"/home/user/bank/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/user/bank/data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['y'].value_counts())\n",
    "print(f\"\\nPositive rate: {train_df['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_train=True):\n",
    "    \"\"\"\n",
    "    特徴量エンジニアリング関数（ワンホットエンコーディング版）\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        処理対象のデータフレーム\n",
    "    is_train : bool\n",
    "        訓練データの場合True、テストデータの場合False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        特徴量エンジニアリング済みのデータフレーム\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== 1. 数値特徴量の変換 =====\n",
    "    # 年齢グループ\n",
    "    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "                              labels=['0-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "    \n",
    "    # balance の対数変換（負の値があるため調整）\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\n",
    "    \n",
    "    # ===== 2. 時系列特徴量 =====\n",
    "    # duration関連\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n",
    "    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\n",
    "    df['duration_log'] = np.log1p(df['duration'])\n",
    "    \n",
    "    # previous関連\n",
    "    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\n",
    "    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\n",
    "    \n",
    "    # ===== 3. 月のマッピングと周期性エンコーディング =====\n",
    "    month_mapping = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }\n",
    "    df['month_numeric'] = df['month'].map(month_mapping)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\n",
    "    \n",
    "    # ===== 4. ローン関連の特徴量 =====\n",
    "    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n",
    "    \n",
    "    # ===== 5. カテゴリカル特徴量の準備 =====\n",
    "    # バイナリ変数を数値化\n",
    "    binary_cols = ['default', 'housing', 'loan']\n",
    "    for col in binary_cols:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    # ワンホットエンコーディング対象のカテゴリカル変数\n",
    "    categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group']\n",
    "    \n",
    "    # ===== 6. 相互作用特徴量（ワンホット化前に作成） =====\n",
    "    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\n",
    "    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "    # 相互作用特徴量もワンホット化対象に追加\n",
    "    interaction_cols = ['job_education', 'contact_month']\n",
    "    categorical_cols.extend(interaction_cols)\n",
    "    \n",
    "    # monthは既に周期性エンコーディングしたので削除\n",
    "    df = df.drop(columns=['month', 'month_numeric'])\n",
    "    \n",
    "    return df, categorical_cols\n",
    "\n",
    "# 特徴量エンジニアリングを適用\n",
    "train_processed, categorical_cols = feature_engineering(train_df, is_train=True)\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False)\n",
    "\n",
    "print(\"特徴量エンジニアリング完了\")\n",
    "print(f\"Train shape: {train_processed.shape}\")\n",
    "print(f\"\\nカテゴリカル変数: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワンホットエンコーディング実行\n",
    "train_encoded = pd.get_dummies(train_processed, columns=categorical_cols, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 訓練データとテストデータのカラムを揃える\n",
    "# テストデータに存在しないカラムを追加（0で埋める）\n",
    "missing_cols = set(train_encoded.columns) - set(test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    if col != 'y':  # ターゲット変数以外\n",
    "        test_encoded[col] = 0\n",
    "\n",
    "# 訓練データに存在しないカラムを削除\n",
    "extra_cols = set(test_encoded.columns) - set(train_encoded.columns)\n",
    "test_encoded = test_encoded.drop(columns=list(extra_cols))\n",
    "\n",
    "# カラムの順序を揃える\n",
    "test_encoded = test_encoded[train_encoded.drop(columns=['y']).columns]\n",
    "\n",
    "print(f\"ワンホットエンコーディング後のTrain shape: {train_encoded.shape}\")\n",
    "print(f\"ワンホットエンコーディング後のTest shape: {test_encoded.shape}\")\n",
    "print(f\"\\n総特徴量数: {train_encoded.shape[1] - 2}\")  # id, yを除く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットと特徴量の分離\n",
    "y = train_encoded['y']\n",
    "X = train_encoded.drop(columns=['id', 'y'])\n",
    "X_test = test_encoded.drop(columns=['id'])\n",
    "\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"訓練データサンプル数: {X.shape[0]}\")\n",
    "print(f\"テストデータサンプル数: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差検証を用いたLightGBMのハイパーパラメータ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb_cv(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化（交差検証版）\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 3000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"class_weight\": \"balanced\",  # 不均衡データ対応\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "    \n",
    "    # 5-Fold Stratified Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    # 平均AUCを返す\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Optuna最適化実行\n",
    "print(\"ハイパーパラメータ最適化を開始します...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_cv\")\n",
    "study_lgb.optimize(objective_lgb_cv, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\nBest params:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適パラメータで交差検証学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適パラメータの設定\n",
    "best_params = study_lgb.best_params.copy()\n",
    "best_params.update({\n",
    "    \"n_estimators\": 3000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"boosting_type\": \"gbdt\"\n",
    "})\n",
    "\n",
    "# 交差検証で学習し、各フォールドの予測を保存\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "cv_scores = []\n",
    "models = []\n",
    "\n",
    "print(\"交差検証で学習を開始します...\\n\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Out-of-Fold予測\n",
    "    oof_predictions[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\n",
    "    \n",
    "    # テストデータ予測（平均を取るため）\n",
    "    test_predictions += model.predict_proba(X_test)[:, 1] / 5\n",
    "    \n",
    "    # スコア計算\n",
    "    fold_auc = roc_auc_score(y_valid_fold, oof_predictions[valid_idx])\n",
    "    cv_scores.append(fold_auc)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} AUC: {fold_auc:.5f}\")\n",
    "    print()\n",
    "\n",
    "# 全体のOOFスコア\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Overall OOF AUC: {overall_auc:.5f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.5f} ± {np.std(cv_scores):.5f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適な閾値を探索\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.3, 0.8, 0.01):\n",
    "    pred_binary = (oof_predictions > threshold).astype(int)\n",
    "    f1 = f1_score(y, pred_binary)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n最適閾値: {best_threshold:.3f}\")\n",
    "print(f\"最適F1スコア: {best_f1:.5f}\")\n",
    "\n",
    "# 最適閾値での評価\n",
    "oof_binary = (oof_predictions > best_threshold).astype(int)\n",
    "print(f\"\\n=== 最適閾値での評価 ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y, oof_binary):.5f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y, oof_binary))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y, oof_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度の可視化（最後のモデルを使用）\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': models[-1].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'][:20], feature_importance['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 重要な特徴量:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータ予測と提出ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適閾値で二値化\n",
    "test_pred_binary = (test_predictions > best_threshold).astype(int)\n",
    "\n",
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/user/bank/data/improved_onehot_cv_submission.csv', index=False, header=False)\n",
    "\n",
    "print(\"提出ファイルを作成しました: improved_onehot_cv_submission.csv\")\n",
    "print(f\"\\n予測分布:\")\n",
    "print(submission['y'].value_counts())\n",
    "print(f\"\\nPositive予測率: {submission['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確率値も保存（閾値調整用）\n",
    "submission_proba = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y_proba': test_predictions,\n",
    "    'y_pred': test_pred_binary\n",
    "})\n",
    "\n",
    "submission_proba.to_csv('/home/user/bank/data/improved_onehot_cv_submission_with_proba.csv', index=False)\n",
    "print(\"確率値付き提出ファイルも作成しました: improved_onehot_cv_submission_with_proba.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
