{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36565312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ライブラリのインポート完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takato/bita/bank/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577f052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (27128, 18)\n",
      "Test shape: (18083, 17)\n",
      "\n",
      "Target distribution:\n",
      "y\n",
      "0    23954\n",
      "1     3174\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive rate: 0.1170\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"/home/takato/bita/bank/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/takato/bita/bank/data/test.csv\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['y'].value_counts())\n",
    "print(f\"\\nPositive rate: {train_df['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5fc420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量エンジニアリング完了\n",
      "Train shape: (27128, 32)\n",
      "\n",
      "カテゴリカル変数: ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group', 'job_education', 'contact_month']\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df, is_train=True):\n",
    "    \"\"\"\n",
    "    特徴量エンジニアリング関数（ワンホットエンコーディング版）\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        処理対象のデータフレーム\n",
    "    is_train : bool\n",
    "        訓練データの場合True、テストデータの場合False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        特徴量エンジニアリング済みのデータフレーム\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== 1. 数値特徴量の変換 =====\n",
    "    # 年齢グループ (スタージェンの公式に基づく最適ビン数: 16)\n",
    "    # k = 1 + 3.322 * log10(n) = 1 + 3.322 * log10(27128) ≈ 16\n",
    "    # 年齢範囲: 18-95歳を16グループに等幅分割\n",
    "    df['age_group'] = pd.cut(df['age'], bins=16).astype(str)\n",
    "    # LightGBMのエラー回避: 特殊文字を置換\n",
    "    df['age_group'] = df['age_group'].str.replace(r'[(),.\\[\\] ]', '_', regex=True)\n",
    "    \n",
    "    # balance の対数変換（負の値があるため調整）\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\n",
    "    \n",
    "    # ===== 2. 時系列特徴量 =====\n",
    "    # duration関連\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n",
    "    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\n",
    "    df['duration_log'] = np.log1p(df['duration'])\n",
    "    \n",
    "    # previous関連\n",
    "    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\n",
    "    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\n",
    "    \n",
    "    # ===== 3. 月のマッピングと周期性エンコーディング =====\n",
    "    month_mapping = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }\n",
    "    df['month_numeric'] = df['month'].map(month_mapping)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\n",
    "    \n",
    "    # ===== 4. ローン関連の特徴量 =====\n",
    "    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n",
    "    \n",
    "    # ===== 5. カテゴリカル特徴量の準備 =====\n",
    "    # バイナリ変数を数値化\n",
    "    binary_cols = ['default', 'housing', 'loan']\n",
    "    for col in binary_cols:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    # ワンホットエンコーディング対象のカテゴリカル変数\n",
    "    categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group']\n",
    "    \n",
    "    # ===== 6. 相互作用特徴量（ワンホット化前に作成） =====\n",
    "    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\n",
    "    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "    # 相互作用特徴量もワンホット化対象に追加\n",
    "    interaction_cols = ['job_education', 'contact_month']\n",
    "    categorical_cols.extend(interaction_cols)\n",
    "    \n",
    "    # monthは既に周期性エンコーディングしたので削除\n",
    "    df = df.drop(columns=['month', 'month_numeric'])\n",
    "    \n",
    "    return df, categorical_cols\n",
    "\n",
    "# 特徴量エンジニアリングを適用\n",
    "train_processed, categorical_cols = feature_engineering(train_df, is_train=True)\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False)\n",
    "\n",
    "print(\"特徴量エンジニアリング完了\")\n",
    "print(f\"Train shape: {train_processed.shape}\")\n",
    "print(f\"\\nカテゴリカル変数: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c7b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ワンホットエンコーディング後のTrain shape: (27128, 142)\n",
      "ワンホットエンコーディング後のTest shape: (18083, 141)\n",
      "\n",
      "総特徴量数: 140\n"
     ]
    }
   ],
   "source": [
    "# ワンホットエンコーディング実行\n",
    "train_encoded = pd.get_dummies(train_processed, columns=categorical_cols, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 訓練データとテストデータのカラムを揃える\n",
    "# テストデータに存在しないカラムを追加（0で埋める）\n",
    "missing_cols = set(train_encoded.columns) - set(test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    if col != 'y':  # ターゲット変数以外\n",
    "        test_encoded[col] = 0\n",
    "\n",
    "# 訓練データに存在しないカラムを削除\n",
    "extra_cols = set(test_encoded.columns) - set(train_encoded.columns)\n",
    "test_encoded = test_encoded.drop(columns=list(extra_cols))\n",
    "\n",
    "# カラムの順序を揃える\n",
    "test_encoded = test_encoded[train_encoded.drop(columns=['y']).columns]\n",
    "\n",
    "print(f\"ワンホットエンコーディング後のTrain shape: {train_encoded.shape}\")\n",
    "print(f\"ワンホットエンコーディング後のTest shape: {test_encoded.shape}\")\n",
    "print(f\"\\n総特徴量数: {train_encoded.shape[1] - 2}\")  # id, yを除く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b621fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量数: 140\n",
      "訓練データサンプル数: 27128\n",
      "テストデータサンプル数: 18083\n"
     ]
    }
   ],
   "source": [
    "# ターゲットと特徴量の分離\n",
    "y = train_encoded['y']\n",
    "X = train_encoded.drop(columns=['id', 'y'])\n",
    "X_test = test_encoded.drop(columns=['id'])\n",
    "\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"訓練データサンプル数: {X.shape[0]}\")\n",
    "print(f\"テストデータサンプル数: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:14,307] A new study created in memory with name: lgbm_cv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ハイパーパラメータ最適化を開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.930762:   2%|▏         | 1/50 [00:01<01:10,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:15,746] Trial 0 finished with value: 0.9307623340311725 and parameters: {'learning_rate': 0.07277859425082907, 'num_leaves': 85, 'max_depth': 5, 'min_child_samples': 63, 'subsample': 0.933372901638837, 'colsample_bytree': 0.7149407266157795, 'reg_alpha': 9.120766108960946e-05, 'reg_lambda': 2.522674416180109e-06, 'min_split_gain': 0.10196802324714072}. Best is trial 0 with value: 0.9307623340311725.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.930762:   4%|▍         | 2/50 [00:03<01:33,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:18,053] Trial 1 finished with value: 0.9304073814759622 and parameters: {'learning_rate': 0.06643664074237393, 'num_leaves': 129, 'max_depth': 3, 'min_child_samples': 85, 'subsample': 0.6707579689923601, 'colsample_bytree': 0.9002223431735961, 'reg_alpha': 0.027096206177567698, 'reg_lambda': 0.6609889641687815, 'min_split_gain': 0.03002438837251109}. Best is trial 0 with value: 0.9307623340311725.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.930762:   6%|▌         | 3/50 [00:08<02:34,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:22,936] Trial 2 finished with value: 0.9296571658876358 and parameters: {'learning_rate': 0.014700743675407987, 'num_leaves': 115, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.8527005234158807, 'colsample_bytree': 0.6112733408313316, 'reg_alpha': 0.0003594555437221908, 'reg_lambda': 2.148566632620408e-06, 'min_split_gain': 0.2554754234176817}. Best is trial 0 with value: 0.9307623340311725.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.930762:   8%|▊         | 4/50 [00:11<02:27,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:26,028] Trial 3 finished with value: 0.9304913611220019 and parameters: {'learning_rate': 0.025700493846096805, 'num_leaves': 135, 'max_depth': 6, 'min_child_samples': 66, 'subsample': 0.7036311007273456, 'colsample_bytree': 0.988844793771696, 'reg_alpha': 2.3960595160229653e-07, 'reg_lambda': 0.0002339265185249398, 'min_split_gain': 0.25935275342085984}. Best is trial 0 with value: 0.9307623340311725.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.930762:  10%|█         | 5/50 [00:12<01:49,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:39:27,082] Trial 4 finished with value: 0.9304345972468789 and parameters: {'learning_rate': 0.09002269207636446, 'num_leaves': 70, 'max_depth': 7, 'min_child_samples': 56, 'subsample': 0.7324497780142275, 'colsample_bytree': 0.868030661891015, 'reg_alpha': 0.6874218052459495, 'reg_lambda': 0.15200375798755866, 'min_split_gain': 0.8546848398897761}. Best is trial 0 with value: 0.9307623340311725.\n"
     ]
    }
   ],
   "source": [
    "def objective_lgb_cv(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化（交差検証版）\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 3000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"class_weight\": \"balanced\",  # 不均衡データ対応\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "    \n",
    "    # 5-Fold Stratified Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    # 平均AUCを返す\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Optuna最適化実行\n",
    "print(\"ハイパーパラメータ最適化を開始します...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_cv\")\n",
    "study_lgb.optimize(objective_lgb_cv, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\nBest params:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "改善版: ワンホットエンコーディング + 交差検証 + LightGBM最適化\n",
    "このノートブックでは以下の改善を実装します:\n",
    "\n",
    "カテゴリ特徴量をワンホットエンコーディングで変換\n",
    "StratifiedKFold交差検証で汎化性能を向上\n",
    "LightGBMのハイパーパラメータ最適化\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv(\"/home/user/bank/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/user/bank/data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['y'].value_counts())\n",
    "print(f\"\\nPositive rate: {train_df['y'].mean():.4f}\")\n",
    "def feature_engineering(df, is_train=True):\n",
    "    \"\"\"\n",
    "    特徴量エンジニアリング関数（ワンホットエンコーディング版）\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        処理対象のデータフレーム\n",
    "    is_train : bool\n",
    "        訓練データの場合True、テストデータの場合False\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        特徴量エンジニアリング済みのデータフレーム\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ===== 1. 数値特徴量の変換 =====\n",
    "    # 年齢グループ (スタージェンの公式に基づく最適ビン数: 16)\n",
    "    # k = 1 + 3.322 * log10(n) = 1 + 3.322 * log10(27128) ≈ 16\n",
    "    # 年齢範囲: 18-95歳を16グループに等幅分割\n",
    "    df['age_group'] = pd.cut(df['age'], bins=16).astype(str)\n",
    "    # LightGBMのエラー回避: 特殊文字を置換\n",
    "    df['age_group'] = df['age_group'].str.replace(r'[(),.\\[\\] ]', '_', regex=True)\n",
    "    \n",
    "    # balance の対数変換（負の値があるため調整）\n",
    "    df['balance_log'] = np.log1p(df['balance'] - df['balance'].min() + 1)\n",
    "    df['balance_positive'] = (df['balance'] > 0).astype(int)\n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\n",
    "    \n",
    "    # ===== 2. 時系列特徴量 =====\n",
    "    # duration関連\n",
    "    df['duration_per_day'] = df['duration'] / (df['day'] + 1)\n",
    "    df['campaign_efficiency'] = df['duration'] / (df['campaign'] + 1)\n",
    "    df['duration_log'] = np.log1p(df['duration'])\n",
    "    \n",
    "    # previous関連\n",
    "    df['has_previous_contact'] = (df['pdays'] != -1).astype(int)\n",
    "    df['previous_per_pdays'] = df['previous'] / (df['pdays'].replace(-1, 1) + 1)\n",
    "    \n",
    "    # ===== 3. 月のマッピングと周期性エンコーディング =====\n",
    "    month_mapping = {\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "        'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "        'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }\n",
    "    df['month_numeric'] = df['month'].map(month_mapping)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_numeric'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_numeric'] / 12)\n",
    "    \n",
    "    # ===== 4. ローン関連の特徴量 =====\n",
    "    df['total_loans'] = (df['housing'] == 'yes').astype(int) + (df['loan'] == 'yes').astype(int)\n",
    "    df['has_any_loan'] = (df['total_loans'] > 0).astype(int)\n",
    "    \n",
    "    # ===== 5. カテゴリカル特徴量の準備 =====\n",
    "    # バイナリ変数を数値化\n",
    "    binary_cols = ['default', 'housing', 'loan']\n",
    "    for col in binary_cols:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "    \n",
    "    # ワンホットエンコーディング対象のカテゴリカル変数\n",
    "    categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome', 'age_group']\n",
    "    \n",
    "    # ===== 6. 相互作用特徴量（ワンホット化前に作成） =====\n",
    "    df['job_education'] = df['job'].astype(str) + '_' + df['education'].astype(str)\n",
    "    df['contact_month'] = df['contact'].astype(str) + '_' + df['month'].astype(str)\n",
    "    \n",
    "    # 相互作用特徴量もワンホット化対象に追加\n",
    "    interaction_cols = ['job_education', 'contact_month']\n",
    "    categorical_cols.extend(interaction_cols)\n",
    "    \n",
    "    # monthは既に周期性エンコーディングしたので削除\n",
    "    df = df.drop(columns=['month', 'month_numeric'])\n",
    "    \n",
    "    return df, categorical_cols\n",
    "\n",
    "# 特徴量エンジニアリングを適用\n",
    "train_processed, categorical_cols = feature_engineering(train_df, is_train=True)\n",
    "test_processed, _ = feature_engineering(test_df, is_train=False)\n",
    "\n",
    "print(\"特徴量エンジニアリング完了\")\n",
    "print(f\"Train shape: {train_processed.shape}\")\n",
    "print(f\"\\nカテゴリカル変数: {categorical_cols}\")\n",
    "# ワンホットエンコーディング実行\n",
    "train_encoded = pd.get_dummies(train_processed, columns=categorical_cols, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 訓練データとテストデータのカラムを揃える\n",
    "# テストデータに存在しないカラムを追加（0で埋める）\n",
    "missing_cols = set(train_encoded.columns) - set(test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    if col != 'y':  # ターゲット変数以外\n",
    "        test_encoded[col] = 0\n",
    "\n",
    "# 訓練データに存在しないカラムを削除\n",
    "extra_cols = set(test_encoded.columns) - set(train_encoded.columns)\n",
    "test_encoded = test_encoded.drop(columns=list(extra_cols))\n",
    "\n",
    "# カラムの順序を揃える\n",
    "test_encoded = test_encoded[train_encoded.drop(columns=['y']).columns]\n",
    "\n",
    "print(f\"ワンホットエンコーディング後のTrain shape: {train_encoded.shape}\")\n",
    "print(f\"ワンホットエンコーディング後のTest shape: {test_encoded.shape}\")\n",
    "print(f\"\\n総特徴量数: {train_encoded.shape[1] - 2}\")  # id, yを除く\n",
    "# ターゲットと特徴量の分離\n",
    "y = train_encoded['y']\n",
    "X = train_encoded.drop(columns=['id', 'y'])\n",
    "X_test = test_encoded.drop(columns=['id'])\n",
    "\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"訓練データサンプル数: {X.shape[0]}\")\n",
    "print(f\"テストデータサンプル数: {X_test.shape[0]}\")\n",
    "交差検証を用いたLightGBMのハイパーパラメータ最適化\n",
    "def objective_lgb_cv(trial):\n",
    "    \"\"\"\n",
    "    LightGBMのハイパーパラメータ最適化（交差検証版）\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "        \"n_estimators\": 3000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"class_weight\": \"balanced\",  # 不均衡データ対応\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "    \n",
    "    # 5-Fold Stratified Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_valid_fold, preds)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    # 平均AUCを返す\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Optuna最適化実行\n",
    "print(\"ハイパーパラメータ最適化を開始します...\")\n",
    "study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_cv\")\n",
    "study_lgb.optimize(objective_lgb_cv, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\nBest params:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "最適パラメータで交差検証学習\n",
    "# 最適パラメータの設定\n",
    "best_params = study_lgb.best_params.copy()\n",
    "best_params.update({\n",
    "    \"n_estimators\": 3000,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"boosting_type\": \"gbdt\"\n",
    "})\n",
    "\n",
    "# 交差検証で学習し、各フォールドの予測を保存\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "cv_scores = []\n",
    "models = []\n",
    "\n",
    "print(\"交差検証で学習を開始します...\\n\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Out-of-Fold予測\n",
    "    oof_predictions[valid_idx] = model.predict_proba(X_valid_fold)[:, 1]\n",
    "    \n",
    "    # テストデータ予測（平均を取るため）\n",
    "    test_predictions += model.predict_proba(X_test)[:, 1] / 5\n",
    "    \n",
    "    # スコア計算\n",
    "    fold_auc = roc_auc_score(y_valid_fold, oof_predictions[valid_idx])\n",
    "    cv_scores.append(fold_auc)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} AUC: {fold_auc:.5f}\")\n",
    "    print()\n",
    "\n",
    "# 全体のOOFスコア\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Overall OOF AUC: {overall_auc:.5f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.5f} ± {np.std(cv_scores):.5f}\")\n",
    "print(f\"{'='*50}\")\n",
    "# 最適な閾値を探索\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.3, 0.8, 0.01):\n",
    "    pred_binary = (oof_predictions > threshold).astype(int)\n",
    "    f1 = f1_score(y, pred_binary)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n最適閾値: {best_threshold:.3f}\")\n",
    "print(f\"最適F1スコア: {best_f1:.5f}\")\n",
    "\n",
    "# 最適閾値での評価\n",
    "oof_binary = (oof_predictions > best_threshold).astype(int)\n",
    "print(f\"\\n=== 最適閾値での評価 ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y, oof_binary):.5f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y, oof_binary))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y, oof_binary))\n",
    "# 特徴量重要度の可視化（最後のモデルを使用）\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': models[-1].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'][:20], feature_importance['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 重要な特徴量:\")\n",
    "print(feature_importance.head(20))\n",
    "テストデータ予測と提出ファイル作成\n",
    "# 最適閾値で二値化\n",
    "test_pred_binary = (test_predictions > best_threshold).astype(int)\n",
    "\n",
    "# 提出ファイル作成\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y': test_pred_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/user/bank/data/improved_onehot_cv_submission.csv', index=False, header=False)\n",
    "\n",
    "print(\"提出ファイルを作成しました: improved_onehot_cv_submission.csv\")\n",
    "print(f\"\\n予測分布:\")\n",
    "print(submission['y'].value_counts())\n",
    "print(f\"\\nPositive予測率: {submission['y'].mean():.4f}\")\n",
    "# 確率値も保存（閾値調整用）\n",
    "submission_proba = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'y_proba': test_predictions,\n",
    "    'y_pred': test_pred_binary\n",
    "})\n",
    "\n",
    "submission_proba.to_csv('/home/user/bank/data/improved_onehot_cv_submission_with_proba.csv', index=False)\n",
    "print(\"確率値付き提出ファイルも作成しました: improved_onehot_cv_submission_with_proba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適な閾値を探索\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.3, 0.8, 0.01):\n",
    "    pred_binary = (oof_predictions > threshold).astype(int)\n",
    "    f1 = f1_score(y, pred_binary)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n最適閾値: {best_threshold:.3f}\")\n",
    "print(f\"最適F1スコア: {best_f1:.5f}\")\n",
    "\n",
    "# 最適閾値での評価\n",
    "oof_binary = (oof_predictions > best_threshold).astype(int)\n",
    "print(f\"\\n=== 最適閾値での評価 ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y, oof_binary):.5f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y, oof_binary))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y, oof_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signate-bank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
